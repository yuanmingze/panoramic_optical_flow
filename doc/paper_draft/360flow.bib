% Encoding: UTF-8

@String{AAAI           = {Proceedings of the Conference on Artificial Intelligence (AAAI)}}
@String{ACCV           = {ACCV}}
@String{ACCVW          = {Proceedings of ACCV Workshops}}
@String{AFRIGRAPH      = {Proceedings of the International Conference on Computer Graphics, Virtual Reality, Visualisation and Interaction in Africa (Afrigraph)}}
@String{APGV           = {Proceedings of the Symposium on Applied Perception in Graphics and Visualization}}
@String{apr            = {April}}
@String{aug            = {August}}
@String{BMVC           = {BMVC}}
@String{CAe            = {CAe}}
@String{CAG            = {Computers \& Graphics}}
@String{CGA            = {IEEE Comput. Graph. Appl.}}
@String{CGF            = {Comput. Graph. Forum}}
@String{CGI            = {Proceedings of Computer Graphics International (CGI)}}
@String{CHI            = {CHI}}
@String{CORL           = {Proceedings of the Conference on Robot Learning (CoRL)}}
@String{CVIU           = {Comput. Vision Image Understanding}}
@String{CVM            = {Computational Visual Media}}
@String{CVMP           = {CVMP}}
@String{CVPR           = {CVPR}}
@String{CVPRW          = {CVPR Workshops}}
@String{DAGM           = {Pattern Recognition (Proceedings of the DAGM Symposium)}}
@String{dec            = {December}}
@String{ECCV           = {ECCV}}
@String{ECCVW          = {ECCV Workshops}}
@String{EGSR           = {Proceedings of the Eurographics Symposium on Rendering}}
@String{EGWR           = {Proceedings of the Eurographics Workshop on Rendering}}
@String{ETRA           = {ETRA}}
@String{feb            = {February}}
@String{FG             = {Proceedings of the International Conference on Automatic Face and Gesture Recognition (FG)}}
@String{GCPR           = {Pattern Recognition (Proceedings of the German Conference on Pattern Recognition)}}
@String{GI             = {Graphics Interface}}
@String{GRAPHITE       = {Proceedings of the International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia (GRAPHITE)}}
@String{HPG            = {Proceedings of High Performance Graphics (HPG)}}
@String{I3D            = {Proceedings of the Symposium on Interactive 3D Graphics and Games (I3D)}}
@String{ICASSP         = {ICASSP}}
@String{ICCP           = {ICCP}}
@String{ICCV           = {ICCV}}
@String{ICCVW          = {ICCV Workshops}}
@String{ICIP           = {ICIP}}
@String{ICLR           = {ICLR}}
@String{ICME           = {ICME}}
@String{ICML           = {Proceedings of the International Conference on Machine Learning (ICML)}}
@String{ICPR           = {ICPR}}
@String{ICRA           = {ICRA}}
@String{IJCV           = {IJCV}}
@String{IROS           = {IROS}}
@String{ISM            = {Proceedings of the International Symposium on Multimedia (ISM)}}
@String{ISMAR          = {ISMAR}}
@String{jan            = {January}}
@String{JCGT           = {Journal of Computer Graphics Techniques (JCGT)}}
@String{jul            = {July}}
@String{jun            = {June}}
@String{LNCS           = {Lecture Notes in Computer Science}}
@String{mar            = {March}}
@String{may            = {May}}
@String{MULTIMEDIA     = {Proceedings of the International Conference on Multimedia}}
@String{NeurIPS        = {NeurIPS}}
@String{NeurIPSW       = {NeurIPS Workshops}}
@String{NIPS           = {NIPS}}
@String{NIPSW          = {NIPS Workshops}}
@String{nov            = {November}}
@String{NPAR           = {Proceedings of the International Symposium on Non-Photorealistic Animation and Rendering (NPAR)}}
@String{oct            = {October}}
@String{PAMI           = {TPAMI}}
@String{Proc3DIM       = {Proceedings of the International Workshop on 3D Digital Imaging and Modeling (3DIM)}}
@String{Proc3DIMPVT    = {Proceedings of the International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT)}}
@String{Proc3DPVT      = {Proceedings of the International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT)}}
@String{Proc3DTV       = {Proceedings of the 3DTV Conference}}
@String{Proc3DV        = {3DV}}
@String{PROCAMS        = {Proceedings of the International Workshop on Projector-Camera Systems (PROCAMS)}}
@String{ProcCGIT       = {Proceedings of the ACM on Computer Graphics and Interactive Techniques}}
@String{ProcCVM        = {Proceedings of Computational Visual Media (CVM)}}
@String{ProcEG         = {Comput. Graph. Forum}}
@String{ProcEGSR       = {Comput. Graph. Forum}}
@String{ProcPG         = {Comput. Graph. Forum}}
@String{ProcSGP        = {Comput. Graph. Forum}}
@String{ProcSIGASIA    = {ACM Trans. Graph.}}
@String{ProcSIGGRAPH   = {ACM Trans. Graph.}}
@String{ProcSIGGRAPHCG = {Computer Graphics (Proceedings of SIGGRAPH)}}
@String{ProcSPIE       = {Proceedings of SPIE}}
@String{RSS            = {Proceedings of Robotics: Science and Systems (RSS)}}
@String{SAP            = {Proceedings of the Symposium on Applied Perception (SAP)}}
@String{SBIM           = {Proceedings of the Symposium on Sketch-Based Interfaces and Modeling (SBIM)}}
@String{SCA            = {Proceedings of the Symposium on Computer Animation (SCA)}}
@String{sep            = {September}}
@String{SIGGRAPH       = {SIGGRAPH}}
@String{TAP            = {ACM Trans. Appl. Percept.}}
@String{TCSVT          = {IEEE Transactions on Circuits and Systems for Video Technology}}
@String{TIP            = {IEEE Transactions on Image Processing}}
@String{TOG            = {ACM Trans. Graph.}}
@String{TPCG           = {Proceedings of Theory and Practice of Computer Graphics (TPCG)}}
@String{TVCG           = {TVCG}}
@String{UIST           = {UIST}}
@String{VMV            = {Proceedings of the Vision, Modeling, and Visualization Workshop (VMV)}}
@String{VR             = {IEEE VR}}
@String{VR3DUI         = {IEEE VR}}
@String{VRST           = {VRST}}
@String{VRW            = {Proceedings of IEEE Virtual Reality Workshops}}
@String{WACV           = {WACV}}
@String{WACVW          = {Proceedings of WACV Workshops}}
@String{WSBIM          = {Proceedings of the Workshop on Sketch-Based Interfaces and Modeling (SBIM)}}
@String{WSCG           = {Proceedings of the International Conference in Central Europe on Computer Graphics, Visualisation and Computer Vision (WSCG)}}

@InProceedings{AleotPM2021,
  author    = {Filippo Aleotti and Matteo Poggi and Stefano Mattoccia},
  title     = {Learning optical flow from still images},
  booktitle = CVPR,
  year      = {2021},
  abstract  = {This paper deals with the scarcity of data for training optical flow networks, highlighting the limitations of existing sources such as labeled synthetic datasets or unlabeled real videos. Specifically, we introduce a framework to generate accurate ground-truth optical flow annotations quickly and in large amounts from any readily available single real picture. Given an image, we use an off-the-shelf monocular depth estimation network to build a plausible point cloud for the observed scene. Then, we virtually move the camera in the reconstructed environment with known motion vectors and rotation angles, allowing us to synthesize both a novel view and the corresponding optical flow field connecting each pixel in the input image to the one in the new frame. When trained with our data, state-of-the-art optical flow networks achieve superior generalization to unseen real data compared to the same models trained either on annotated synthetic datasets or unlabeled videos, and better specialization if combined with synthetic images.},
  arxiv     = {2104.03965},
  _url       = {https://arxiv.org/abs/2104.03965},
}

@Unpublished{ArmenSZS2017,
  author    = {Iro Armeni and Sasha Sax and Amir R. Zamir and Silvio Savarese},
  title     = {Joint {{2D}-3D}-Semantic Data for Indoor Scene Understanding},
  note      = {arXiv:\href{https://arxiv.org/abs/1702.01105}{1702.01105}},
  year      = {2017},
  abstract  = {We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360° equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: http://3Dsemantics.stanford.edu/},
  arxiv     = {1702.01105},
  _url       = {https://github.com/alexsax/2D-3D-Semantics},
}

@InProceedings{ArmenSZJBFS2016,
  author    = {Iro Armeni and Ozan Sener and Amir R. Zamir and Helen Jiang and Ioannis Brilakis and Martin Fischer and Silvio Savarese},
  title     = {{3D} Semantic Parsing of Large-Scale Indoor Spaces},
  booktitle = CVPR,
  year      = {2016},
  _month     = jun,
  abstract  = {In this paper, we propose a method for semantic parsing the 3D point cloud of an entire building using a hierarchical approach: first, the raw data is parsed into semantically meaningful spaces (e.g. rooms, etc) that are aligned into a canonical reference coordinate system. Second, the spaces are parsed into their structural and building elements (e.g. walls, columns, etc). Performing these with a strong notation of global 3D space is the backbone of our method. The alignment in the first step injects strong 3D priors from the canonical coordinate system into the second step for dis- covering elements. This allows diverse challenging scenarios as man-made indoor spaces often show recurrent geo- metric patterns while the appearance features can change drastically. We also argue that identification of structural elements in indoor spaces is essentially a detection problem, rather than segmentation which is commonly used. We evaluated our method on a new dataset of several buildings with a covered area of over 6,000 m2 and over 215 million points, demonstrating robust results readily useful for practical applications.},
}

@InProceedings{ArtizZAD2021,
  author    = {Charles-Olivier Artizzu and Haozhou Zhang and Guillaume Allibert and Cédric Demonceaux},
  title     = {{OmniFlowNet}: a Perspective Neural Network Adaptation for Optical Flow Estimation in Omnidirectional Images},
  booktitle = ICPR,
  year      = {2021},
  _month     = jan,
  abstract  = {Spherical cameras and the latest image processing techniques open up new horizons. In particular, methods based on Convolutional Neural Networks (CNNs) now give excellent results for optical flow estimation on perspective images. However, these approaches are highly dependent on their architectures and training datasets. This paper proposes to benefit from years of improvement in perspective images optical flow estimation and to apply it to omnidirectional ones without training on new datasets. Our network, OmniFlowNet, is built on a CNN specialized in perspective images. Its convolution operation is adapted to be consistent with the equirectangular projection. Tested on spherical datasets created with Blender 1 and several equirectangular videos realized from real indoor and outdoor scenes, OmniFlowNet shows better performance than its original network without extra training.},
  _url       = {https://hal.archives-ouvertes.fr/hal-02968191},
}

@Article{BakerSLRBS2011,
  author    = {Simon Baker and Daniel Scharstein and J. Lewis and Stefan Roth and Michael Black and Richard Szeliski},
  title     = {A Database and Evaluation Methodology for Optical Flow},
  journal   = IJCV,
  year      = {2011},
  volume    = {92},
  number    = {1},
  pages     = {1--31},
  _issn      = {0920-5691},
  note      = {10.1007/s11263-010-0390-2},
  abstract  = {The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1) sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2) realistic synthetic sequences, (3) high frame-rate video used to study interpolation error, and (4) modified stereo sequences of static scenes. In addition to the average angular error used by Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/. Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them.},
  doi       = {10.1007/s11263-010-0390-2},
  issue     = {1},
}

@Article{BerteYLR2020,
  author    = {Tobias Bertel and Mingze Yuan and Reuben Lindroos and Christian Richardt},
  title     = {{OmniPhotos}: Casual 360° {VR} Photography},
  journal   = ProcSIGASIA,
  year      = {2020},
  volume    = {39},
  number    = {6},
  pages     = {267:1--12},
  _month     = dec,
  _issn      = {0730-0301},
  abstract  = {Virtual reality headsets are becoming increasingly popular, yet it remains difficult for casual users to capture immersive 360° VR panoramas. State-of-the-art approaches require capture times of usually far more than a minute and are often limited in their supported range of head motion. We introduce OmniPhotos, a novel approach for quickly and casually capturing high-quality 360° panoramas with motion parallax. Our approach requires a single sweep with a consumer 360° video camera as input, which takes less than 3 seconds to capture with a rotating selfie stick or 10 seconds handheld. This is the fastest capture time for any VR photography approach supporting motion parallax by an order of magnitude. We improve the visual rendering quality of our OmniPhotos by alleviating vertical distortion using a novel deformable proxy geometry, which we fit to a sparse 3D reconstruction of captured scenes. In addition, the 360° input views significantly expand the available viewing area, and thus the range of motion, compared to previous approaches. We have captured more than 50 OmniPhotos and show video results for a large variety of scenes. We will make our code available.},
  doi       = {10.1145/3414685.3417770},
  _url       = {https://richardt.name/omniphotos/},
}

@InProceedings{BhandZY2021,
  author    = {Keshav Bhandari and Ziliang Zong and Yan Yan},
  title     = {Revisiting Optical Flow Estimation in 360 Videos},
  booktitle = ICPR,
  year      = {2021},
  abstract  = {Nowadays 360 video analysis has become a significant research topic in the field since the appearance of high-quality and low-cost 360 wearable devices. In this paper, we propose a novel LiteFlowNet360 architecture for 360 videos optical flow estimation. We design LiteFlowNet360 as a domain adaptation framework from perspective video domain to 360 video domain. We adapt it from simple kernel transformation techniques inspired by Kernel Transformer Network (KTN) to cope with inherent distortion in 360 videos caused by the sphere-to-plane projection. First, we apply an incremental transformation of convolution layers in feature pyramid network and show that further transformation in inference and regularization layers are not important, hence reducing the network growth in terms of size and computation cost. Second, we refine the network by training with augmented data in a supervised manner. We perform data augmentation by projecting the images in a sphere and re-projecting to a plane. Third, we train LiteFlowNet360 in a self-supervised manner using target domain 360 videos. Experimental results show the promising results of 360 video optical flow estimation using the proposed novel architecture.},
  arxiv     = {2010.08045},
  _url       = {https://arxiv.org/abs/2010.08045},
}

@InProceedings{BroxBPW2004,
  author    = {Thomas Brox and Andrés Bruhn and Nils Papenberg and Joachim Weickert},
  title     = {High Accuracy Optical Flow Estimation Based on a Theory for Warping},
  booktitle = ECCV,
  year      = {2004},
  volume    = {3024},
  _series    = LNCS,
  pages     = {25--36},
  _month     = may,
  abstract  = {We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise.},
  doi       = {10.1007/978-3-540-24673-2_3},
}

@InProceedings{ButleWSB2012,
  author    = {Daniel J. Butler and Jonas Wulff and Garrett B. Stanley and Michael J. Black},
  title     = {A Naturalistic Open Source Movie for Optical Flow Evaluation},
  booktitle = ECCV,
  year      = {2012},
  _editor    = {Andrew Fitzgibbon and Svetlana Lazebnik and Pietro Perona and Yoichi Sato and Cordelia Schmid},
  volume    = {7577},
  _series    = LNCS,
  pages     = {611--625},
  abstract  = {Ground truth optical flow is difficult to measure in real scenes with natural motion. As a result, optical flow data sets are restricted in terms of size, complexity, and diversity, making optical flow algorithms difficult to train and test on realistic data. We introduce a new optical flow data set derived from the open source 3D animated short film Sintel. This data set has important features not present in the popular Middlebury flow evaluation: long sequences, large motions, specular reflections, motion blur, defocus blur, and atmospheric effects. Because the graphics data that generated the movie is open source, we are able to render scenes under conditions of varying complexity to evaluate where existing flow algorithms fail. We evaluate several recent optical flow algorithms and find that current highly-ranked methods on the Middlebury evaluation have difficulty with this more complex data set suggesting further research on optical flow estimation is needed. To validate the use of synthetic data, we compare the image- and flow-statistics of Sintel to those of real films and videos and show that they are similar. The data set, metrics, and evaluation website are publicly available.},
  doi       = {10.1007/978-3-642-33783-3_44},
  _isbn      = {978-3-642-33782-6},
  _url       = {http://sintel.is.tue.mpg.de/},
}

@InProceedings{ChangDFHNSSZZ2017,
  author    = {Angel Chang and Angela Dai and Thomas Funkhouser and Maciej Halber and Matthias Nießner and Manolis Savva and Shuran Song and Andy Zeng and Yinda Zhang},
  title     = {{Matterport3D}: Learning from {RGB-D} Data in Indoor Environments},
  booktitle = Proc3DV,
  year      = {2017},
  pages     = {667--676},
  abstract  = {Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.},
  arxiv     = {1709.06158},
  doi       = {10.1109/3DV.2017.00081},
  _url       = {https://github.com/niessner/Matterport},
}

@Article{ChenLFLCG2021,
  author    = {Hong-Xiang Chen and Kunhong Li and Zhiheng Fu and Mengyi Liu and Zonghao Chen and Yulan Guo},
  title     = {Distortion-Aware Monocular Depth Estimation for Omnidirectional Images},
  journal   = {IEEE Signal Processing Letters},
  year      = {2021},
  volume    = {28},
  pages     = {334--338},
  _month     = jan,
  _issn      = {1558-2361},
  abstract  = {Image distortion is a main challenge for tasks on panoramas. In this work, we propose a Distortion-Aware Monocular Omnidirectional (DAMO) network to estimate dense depth maps from indoor panoramas. First, we introduce a distortion-aware module to extract semantic features from omnidirectional images. Specifically, we exploit deformable convolution to adjust its sampling grids to geometric distortions on panoramas. We also utilize a strip pooling module to sample against horizontal distortion introduced by inverse gnomonic projection. Second, we introduce a plug-and-play spherical-aware weight matrix for our loss function to handle the uneven distribution of areas projected from a sphere. Experiments on the 360D dataset show that the proposed method can effectively extract semantic features from distorted panoramas and alleviate the supervision bias caused by distortion. It achieves the state-of-the-art performance on the 360D dataset with high efficiency.},
  arxiv     = {2010.08942},
  doi       = {10.1109/LSP.2021.3050712},
}

@InProceedings{CohenGKW2018,
  author    = {Taco S. Cohen and Mario Geiger and Jonas Koehler and Max Welling},
  title     = {Spherical {CNNs}},
  booktitle = ICLR,
  year      = {2018},
  abstract  = {Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.

In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.},
  arxiv     = {1801.10130},
  _url       = {https://arxiv.org/abs/1801.10130},
}

@InProceedings{CoorsCG2018,
  author    = {Benjamin Coors and Alexandru Paul Condurache and Andreas Geiger},
  title     = {{SphereNet}: Learning Spherical Representations for Detection and Classification in Omnidirectional Images},
  booktitle = ECCV,
  year      = {2018},
  pages     = {518--533},
  _month     = sep,
  abstract  = {Omnidirectional cameras offer great benefits over classical cameras wherever a wide field of view is essential, such as in virtual reality applications or in autonomous robots. Unfortunately, standard convolutional neural networks are not well suited for this scenario as the natural projection surface is a sphere which cannot be unwrapped to a plane without introducing significant distortions, particularly in the polar regions. In this work, we present SphereNet, a novel deep learning framework which encodes invariance against such distortions explicitly into convolutional neural networks. Towards this goal, SphereNet adapts the sampling locations of the convolutional filters, effectively reversing distortions, and wraps the filters around the sphere. By building on regular convolutions, SphereNet enables the transfer of existing perspective convolutional neural network models to the omnidirectional case. We demonstrate the effectiveness of our method on the tasks of image classification and object detection, exploiting two newly created semi-synthetic and real-world omnidirectional datasets.},
  doi       = {10.1007/978-3-030-01240-3_32},
  _isbn      = {978-3-030-01240-3},
  _url       = {http://openaccess.thecvf.com/content_ECCV_2018/html/Benjamin_Coors_SphereNet_Learning_Spherical_ECCV_2018_paper.html},
}

@InProceedings{DosovFIHHGSCB2015,
  author    = {Alexey Dosovitskiy and Philipp Fischer and Eddy Ilg and Philip Hausser and Caner Hazirbas and Vladimir Golkov and Patrick van der Smagt and Daniel Cremers and Thomas Brox},
  title     = {{FlowNet}: Learning Optical Flow With Convolutional Networks},
  booktitle = ICCV,
  year      = {2015},
  _month     = dec,
  abstract  = {Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.},
}

@InProceedings{EderMG2019,
  author    = {Marc Eder and Pierre Moulon and Li Guan},
  title     = {Pano Popups: Indoor {3D} Reconstruction with a Plane-Aware Network},
  booktitle = Proc3DV,
  year      = {2019},
  pages     = {76--84},
  abstract  = {In this work we present a method to train a plane-aware convolutional neural network for dense depth and surface normal estimation as well as plane boundaries from a single indoor 360 image. Using our proposed loss function, our network outperforms existing methods for single-view, indoor, omnidirectional depth estimation and provides an initial benchmark for surface normal prediction from 360 images. Our improvements are due to the use of a novel plane-aware loss that leverages principal curvature as an indicator of planar boundaries. We also show that including geodesic coordinate maps as network priors provides a significant boost in surface normal prediction accuracy. Finally, we demonstrate how we can combine our network's outputs to generate high quality 3D "pop-up" models of indoor scenes.},
  arxiv     = {1907.00939},
  doi       = {10.1109/3DV.2019.00018},
  _issn      = {2475-7888},
  _url       = {https://arxiv.org/abs/1907.00939},
}

@Unpublished{EderPVBF2019,
  author    = {Marc Eder and True Price and Thanh Vu and Akash Bapat and Jan-Michael Frahm},
  title     = {Mapped Convolutions},
  note      = {arXiv:\href{https://arxiv.org/abs/1906.11096}{1906.11096}},
  year      = {2019},
  abstract  = {We present a versatile formulation of the convolution operation that we term a "mapped convolution." The standard convolution operation implicitly samples the pixel grid and computes a weighted sum. Our mapped convolution decouples these two components, freeing the operation from the confines of the image grid and allowing the kernel to process any type of structured data. As a test case, we demonstrate its use by applying it to dense inference on spherical data. We perform an in-depth study of existing spherical image convolution methods and propose an improved sampling method for equirectangular images. Then, we discuss the impact of data discretization when deriving a sampling function, highlighting drawbacks of the cube map representation for spherical data. Finally, we illustrate how mapped convolutions enable us to convolve directly on a mesh by projecting the spherical image onto a geodesic grid and training on the textured mesh. This method exceeds the state of the art for spherical depth estimation by nearly 17%. Our findings suggest that mapped convolutions can be instrumental in expanding the application scope of convolutional neural networks.},
  arxiv     = {1906.11096},
  _url       = {https://github.com/meder411/MappedConvolutions},
}

@InProceedings{EderSLF2020,
  author    = {Marc Eder and Mykhailo Shvets and John Lim and Jan-Michael Frahm},
  title     = {Tangent Images for Mitigating Spherical Distortion},
  booktitle = CVPR,
  year      = {2020},
  abstract  = {In this work, we propose "tangent images," a spherical image representation that facilitates transferable and scalable 360° computer vision. Inspired by techniques in cartography and computer graphics, we render a spherical image to a set of distortion-mitigated, locally-planar image grids tangent to a subdivided icosahedron. By varying the resolution of these grids independently of the subdivision level, we can effectively represent high resolution spherical images while still benefiting from the low-distortion icosahedral spherical approximation. We show that training standard convolutional neural networks on tangent images compares favorably to the many specialized spherical convolutional kernels that have been developed, while also allowing us to scale training to significantly higher spherical resolutions. Furthermore, because we do not require specialized kernels, we show that we can transfer networks trained on perspective images to spherical data without fine-tuning and with limited performance drop-off. Finally, we demonstrate that tangent images can be used to improve the quality of sparse feature detection on spherical images, illustrating its usefulness for traditional computer vision tasks like structure-from-motion and SLAM.},
  arxiv     = {1912.09390},
  _url       = {https://github.com/meder411/Tangent-Images},
}

@Article{FernaFPDCG2020,
  author    = {Clara Fernandez-Labrador and Jose M. Facil and Alejandro Perez-Yus and Cédric Demonceaux and Javier Civera and Jose J. Guerrero},
  title     = {Corners for Layout: End-to-End Layout Recovery from 360 Images},
  journal   = {IEEE Robotics and Automation Letters},
  year      = {2020},
  volume    = {5},
  number    = {2},
  pages     = {1255--1262},
  _month     = apr,
  _issn      = {2377-3766},
  abstract  = {The problem of 3D layout recovery in indoor scenes has been a core research topic for over a decade. However, there are still several major challenges that remain unsolved. Among the most relevant ones, a major part of the state-of-the-art methods make implicit or explicit assumptions on the scenes -e.g. box-shaped or Manhattan layouts. Also, current methods are computationally expensive and not suitable for real-time applications like robot navigation and AR/YR. In this work we present CFL (Corners for Layout), the first end-to-end model that predicts layout corners for 3D layout recovery on 360° images. Our experimental results show that we outperform the state of the art, making less assumptions on the scene than other works, and with lower cost. We also show that our model generalizes better to camera position variations than conventional approaches by using EquiConvs, a convolution applied directly on the spherical projection and hence invariant to the equirectangular distortions.},
  arxiv     = {1903.08094},
  doi       = {10.1109/LRA.2020.2967274},
  _url       = {https://cfernandezlab.github.io/CFL/},
}

@Article{HornS1981,
  author    = {Berthold K. P. Horn and Brian G. Schunck},
  title     = {Determining optical flow},
  journal   = {Artificial Intelligence},
  year      = {1981},
  volume    = {17},
  pages     = {185--203},
  _month     = aug,
  _issn      = {0004-3702},
  abstract  = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.},
  doi       = {10.1016/0004-3702(81)90024-2},
}

@InProceedings{IlgMSKDB2017,
  author    = {Eddy Ilg and Nikolaus Mayer and Tonmoy Saikia and Margret Keuper and Alexey Dosovitskiy and Thomas Brox},
  title     = {{FlowNet} 2.0: Evolution of Optical Flow Estimation with Deep Networks},
  booktitle = CVPR,
  year      = {2017},
  abstract  = {The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a sub-network specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.},
  arxiv     = {1612.01925},
  doi       = {10.1109/CVPR.2017.179},
  _url       = {https://lmb.informatik.uni-freiburg.de/Publications/2017/IMKDB17/},
}

@InProceedings{KroegTDV2016,
  author    = {Till Kroeger and Radu Timofte and Dengxin Dai and Van Gool, Luc},
  title     = {Fast Optical Flow Using Dense Inverse Search},
  booktitle = ECCV,
  year      = {2016},
  _editor    = {Bastian Leibe and Jiri Matas and Nicu Sebe and Max Welling},
  pages     = {471--488},
  abstract  = {Most recent works in optical flow extraction focus on the accuracy and neglect the time complexity. However, in real-life visual applications, such as tracking, activity detection and recognition, the time complexity is critical. We propose a solution with very low time complexity and competitive accuracy for the computation of dense optical flow. It consists of three parts: (1) inverse search for patch correspondences; (2) dense displacement field creation through patch aggregation along multiple scales; (3) variational refinement. At the core of our Dense Inverse Search-based method (DIS) is the efficient search of correspondences inspired by the inverse compositional image alignment proposed by Baker and Matthews (2001, 2004). DIS is competitive on standard optical flow benchmarks. DIS runs at 300 Hz up to 600 Hz on a single CPU core (1024×436 resolution. 42 Hz/46 Hz when including preprocessing: disk access, image re-scaling, gradient computation. More details in Sect. 3.1.), reaching the temporal resolution of human’s biological vision system. It is order(s) of magnitude faster than state-of-the-art methods in the same range of accuracy, making DIS ideal for real-time applications.},
  arxiv     = {1603.03590},
  doi       = {10.1007/978-3-319-46493-0_29},
  _isbn      = {978-3-319-46493-0},
}

@InProceedings{SavvaKMZWJSLKMPB2019,
  author    = {Manolis Savva and Abhishek Kadian and Oleksandr Maksymets and Yili Zhao and Erik Wijmans and Bhavana Jain and Julian Straub and Jia Liu and Vladlen Koltun and Jitendra Malik and Devi Parikh and Dhruv Batra},
  title     = {{Habitat}: A Platform for Embodied {AI} Research},
  booktitle = ICCV,
  year      = {2019},
  pages     = {9339--9347},
  _month     = oct,
  abstract  = {We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments  train, test  x  Matterport3D, Gibson  for multiple sensors  blind, RGB, RGBD, D  and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.},
  doi       = {10.1109/ICCV.2019.00943},
  _issn      = {1550-5499},
  _url       = {http://openaccess.thecvf.com/content_ICCV_2019/html/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.html},
}

@Unpublished{StrauWMCWGEMRVCYBYPYZLCBGMPSBSNGLN2019,
  author    = {Julian Straub and Thomas Whelan and Lingni Ma and Yufan Chen and Erik Wijmans and Simon Green and Jakob J. Engel and Raul Mur-Artal and Carl Ren and Shobhit Verma and Anton Clarkson and Mingfei Yan and Brian Budge and Yajie Yan and Xiaqing Pan and June Yon and Yuyang Zou and Kimberly Leon and Nigel Carter and Jesus Briales and Tyler Gillingham and Elias Mueggler and Luis Pesqueira and Manolis Savva and Dhruv Batra and Hauke M. Strasdat and Renzo De Nardi and Michael Goesele and Steven Lovegrove and Richard Newcombe},
  title     = {The {Replica} Dataset: A Digital Replica of Indoor Spaces},
  note      = {arXiv:\href{https://arxiv.org/abs/1906.05797}{1906.05797}},
  year      = {2019},
  abstract  = {We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world - for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is `Habitat-compatible', i.e. can be natively used with AI Habitat for training and testing embodied agents.},
  arxiv     = {1906.05797},
  _url       = {https://github.com/facebookresearch/Replica-Dataset},
}

@InProceedings{SuG2019,
  author    = {Yu-Chuan Su and Kristen Grauman},
  title     = {Kernel Transformer Networks for Compact Spherical Convolution},
  booktitle = CVPR,
  year      = {2019},
  pages     = {9442--9451},
  _month     = jun,
  abstract  = {Ideally, 360° imagery could inherit the deep convolutional neural networks (CNNs) already trained with great success on perspective projection images. However, existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. In this work, we present the Kernel Transformer Network (KTN). KTNs efficiently transfer convolution kernels from perspective images to the equirectangular projection of 360° images. Given a source CNN for perspective images as input, the KTN produces a function parameterized by a polar angle and kernel as output. Given a novel 360° image, that function in turn can compute convolutions for arbitrary layers and kernels as would the source CNN on the corresponding tangent plane projections. Distinct from all existing methods, KTNs allow model transfer: the same model can be applied to different source CNNs with the same base architecture. This enables application to multiple recognition tasks without re-training the KTN. Validating our approach with multiple source CNNs and datasets, we show that KTNs improve the state of the art for spherical convolution. KTNs successfully preserve the source CNN's accuracy, while offering transferability, scalability to typical image resolutions, and, in many cases, a substantially lower memory footprint.},
  arxiv     = {1812.03115},
  doi       = {10.1109/CVPR.2019.00967},
  _issn      = {1063-6919},
  _url       = {https://github.com/sammy-su/KernelTransformerNetwork},
}

@InProceedings{SunSC2021,
  author        = {Cheng Sun and Min Sun and Hwann-Tzong Chen},
  title         = {{HoHoNet}: 360 Indoor Holistic Understanding with Latent Horizontal Features},
  booktitle     = CVPR,
  year          = {2021},
  abstract      = {We present HoHoNet, a versatile and efficient framework for holistic understanding of an indoor 360-degree panorama using a Latent Horizontal Feature (LHFeat). The compact LHFeat flattens the features along the vertical direction and has shown success in modeling per-column modality for room layout reconstruction. HoHoNet advances in two important aspects. First, the deep architecture is redesigned to run faster with improved accuracy. Second, we propose a novel horizon-to-dense module, which relaxes the per-column output shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones respectively, for modeling dense modalities from a high-resolution $512 \times 1024$ panorama. HoHoNet is also accurate. On the tasks of layout estimation and semantic segmentation, HoHoNet achieves results on par with current state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior arts by a large margin.},
  arxiv         = {2011.11498},
  _url           = {https://github.com/sunset1995/HoHoNet},
}

@InProceedings{SunRB2010,
  author    = {Deqing Sun and S. Roth and M.J. Black},
  title     = {Secrets of optical flow estimation and their principles},
  booktitle = CVPR,
  year      = {2010},
  pages     = {2432--2439},
  _month     = jun,
  abstract  = {The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that #x201C;classical #x201D; flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. Moreover, we find that while median filtering of intermediate flow fields during optimization is a key to recent performance gains, it leads to higher energy solutions. To understand the principles behind this phenomenon, we derive a new objective that formalizes the median filtering heuristic. This objective includes a nonlocal term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that ranks at the top of the Middlebury benchmark.},
  doi       = {10.1109/CVPR.2010.5539939},
  _issn      = {1063-6919},
}

@Article{SunRB2014,
  author    = {Deqing Sun and Stefan Roth and Michael J. Black},
  title     = {A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them},
  journal   = IJCV,
  year      = {2014},
  volume    = {106},
  number    = {2},
  pages     = {115--137},
  _month     = jan,
  _issn      = {0920-5691},
  abstract  = {The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that “classical” flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. One key implementation detail is the median filtering of intermediate flow fields during optimization. While this improves the robustness of classical methods it actually leads to higher energy solutions, meaning that these methods are not optimizing the original objective function. To understand the principles behind this phenomenon, we derive a new objective function that formalizes the median filtering heuristic. This objective function includes a non-local smoothness term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that can better preserve motion details. To take advantage of the trend towards video in wide-screen format, we further introduce an asymmetric pyramid downsampling scheme that enables the estimation of longer range horizontal motions. The methods are evaluated on the Middlebury, MPI Sintel, and KITTI datasets using the same parameter settings.},
  doi       = {10.1007/s11263-013-0644-x},
  keywords  = {optical flow estimation, practices, median filtering, non-local term, motion boundary},
}

@InProceedings{SunSB2010,
  author    = {Deqing Sun and Erik B. Sudderth and Michael J. Black},
  title     = {Layered image motion with explicit occlusions, temporal consistency, and depth ordering},
  booktitle = NIPS,
  year      = {2010},
  pages     = {2226--2234},
  abstract  = {Layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other. For image motion estimation, such models have a long history but have not achieved the wide use or accuracy of non-layered methods. We present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches. In particular, we define a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation. Additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an MRF with a robust spatial prior; the resulting model allows roughness in layers. Finally, a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent models for static scene segmentation. The method achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions.},
  _url       = {http://papers.nips.cc/paper/4030-layered-image-motion-with-explicit-occlusions-temporal-consistency-and-depth-ordering},
}

@InProceedings{SunSP2015,
  author    = {Deqing Sun and Erik B. Sudderth and Hanspeter Pfister},
  title     = {Layered {RGBD} Scene Flow Estimation},
  booktitle = CVPR,
  year      = {2015},
  pages     = {548--556},
  _month     = jun,
  abstract  = {As consumer depth sensors become widely available, estimating scene flow from RGBD sequences has received increasing attention. Although the depth information allows the recovery of 3D motion from a single view, it poses new challenges. In particular, depth boundaries are not well-aligned with RGB image edges and therefore not reliable cues to localize 2D motion boundaries. In addition, methods that extend the 2D optical flow formulation to 3D still produce large errors in occlusion regions. To better use depth for occlusion reasoning, we propose a layered RGBD scene flow method that jointly solves for the scene segmentation and the motion. Our key observation is that the noisy depth is sufficient to decide the depth ordering of layers, thereby avoiding a computational bottleneck for RGB layered methods. Furthermore, the depth enables us to estimate a per-layer 3D rigid motion to constrain the motion of each layer. Experimental results on both the Middlebury and real-world sequences demonstrate the effectiveness of the layered approach for RGBD scene flow estimation.},
  doi       = {10.1109/CVPR.2015.7298653},
}

@Article{SunYLK2020,
  author    = {Deqing Sun and Xiaodong Yang and Ming-Yu Liu and Jan Kautz},
  title     = {Models Matter, So Does Training: An Empirical Study of {CNNs} for Optical Flow Estimation},
  journal   = PAMI,
  year      = {2020},
  volume    = {42},
  number    = {6},
  pages     = {1408--1423},
  _month     = jun,
  _issn      = {1939-3539},
  abstract  = {We investigate two crucial and closely-related aspects of CNNs for optical flow estimation: models and training. First, we design a compact but effective CNN model, called PWC-Net, according to simple and well-established principles: pyramidal processing, warping, and cost volume processing. PWC-Net is 17 times smaller in size, 2 times faster in inference, and 11 percent more accurate on Sintel final than the recent FlowNet2 model. It is the winning entry in the optical flow competition of the robust vision challenge. Next, we experimentally analyze the sources of our performance gains. In particular, we use the same training procedure for PWC-Net to retrain FlowNetC, a sub-network of FlowNet2. The retrained FlowNetC is 56 percent more accurate on Sintel final than the previously trained one and even 5 percent more accurate than the FlowNet2 model. We further improve the training procedure and increase the accuracy of PWC-Net on Sintel by 10 percent and on KITTI 2012 and 2015 by 20 percent. Our newly trained model parameters and training protocols are available on https://github.com/NVlabs/PWC-Net.},
  arxiv     = {1809.05571},
  doi       = {10.1109/TPAMI.2019.2894353},
  _url       = {https://arxiv.org/abs/1809.05571},
}

@InProceedings{SunYLK2018,
  author    = {Deqing Sun and Xiaodong Yang and Ming-Yu Liu and Jan Kautz},
  title     = {{PWC-Net}: {CNNs} for Optical Flow Using Pyramid, Warping, and Cost Volume},
  booktitle = CVPR,
  year      = {2018},
  pages     = {8934--8943},
  _month     = jun,
  abstract  = {We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the current optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our models are available on our project website.},
  arxiv     = {1709.02371},
  doi       = {10.1109/CVPR.2018.00931},
  _url       = {http://openaccess.thecvf.com/content_cvpr_2018/html/Sun_PWC-Net_CNNs_for_CVPR_2018_paper.html},
}

@InProceedings{TeedD2020a,
  author    = {Zachary Teed and Jia Deng},
  title     = {{RAFT}: Recurrent All-Pairs Field Transforms for Optical Flow},
  booktitle = ECCV,
  year      = {2020},
  abstract  = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10\%, a 16\% error reduction from the best published result (6.10\%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30\% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT},
  arxiv     = {2003.12039},
  doi       = {10.1007/978-3-030-58536-5_24},
  _url       = {https://github.com/princeton-vl/RAFT},
}

@Article{Tsai1987,
  author    = {Roger Y. Tsai},
  title     = {A versatile camera calibration technique for high-accuracy {3D} machine vision metrology using off-the-shelf {TV} cameras and lenses},
  journal   = {IEEE Journal on Robotics and Automation},
  year      = {1987},
  volume    = {3},
  number    = {4},
  pages     = {323--344},
  _month     = aug,
  _issn      = {2374-8710},
  abstract  = {A new technique for three-dimensional (3D) camera calibration for machine vision metrology using off-the-shelf TV cameras and lenses is described. The two-stage technique is aimed at efficient computation of camera external position and orientation relative to object reference coordinate system as well as the effective focal length, radial lens distortion, and image scanning parameters. The two-stage technique has advantage in terms of accuracy, speed, and versatility over existing state of the art. A critical review of the state of the art is given in the beginning. A theoretical framework is established, supported by comprehensive proof in five appendixes, and may pave the way for future research on 3D robotics vision. Test results using real data are described. Both accuracy and speed are reported. The experimental results are analyzed and compared with theoretical prediction. Recent effort indicates that with slight modification, the two-stage calibration can be done in real time.},
  doi       = {10.1109/JRA.1987.1087109},
}

@InProceedings{WangYSCT2020,
  author    = {Fu-En Wang and Yu-Hsuan Yeh and Min Sun and Wei-Chen Chiu and Yi-Hsuan Tsai},
  title     = {{BiFuse}: Monocular 360 Depth Estimation via Bi-Projection Fusion},
  booktitle = CVPR,
  year      = {2020},
  pages     = {462--471},
  _month     = jun,
  abstract  = {Depth estimation from a monocular 360 image is an emerging problem that gains popularity due to the availability of consumer-level 360 cameras and the complete surrounding sensing capability. While the standard of 360 imaging is under rapid development, we propose to predict the depth map of a monocular 360 image by mimicking both peripheral and foveal vision of the human eye. To this end, we adopt a two-branch neural network leveraging two common projections: equirectangular and cubemap projections. In particular, equirectangular projection incorporates a complete field-of-view but introduces distortion, whereas cubemap projection avoids distortion but introduces discontinuity at the boundary of the cube. Thus we propose a bi-projection fusion scheme along with learnable masks to balance the feature map from the two projections. Moreover, for the cubemap projection, we propose a spherical padding procedure which mitigates discontinuity at the boundary of each face. We apply our method to four panorama datasets and show favorable results against the existing state-of-the-art methods.},
  doi       = {10.1109/CVPR42600.2020.00054},
  _url       = {https://fuenwang.ml/project/bifuse/},
}

@Article{WangBSS2004,
  author    = {Zhou Wang and Alan C. Bovik and Hamid R. Sheikh and Eero P. Simoncelli},
  title     = {Image quality assessment: from error visibility to structural similarity},
  journal   = TIP,
  year      = {2004},
  volume    = {13},
  number    = {4},
  pages     = {600--612},
  _month     = apr,
  abstract  = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  doi       = {10.1109/TIP.2003.819861},
  keywords  = {error sensitivity, human visual system (HVS), image coding, image quality assessment, JPEG, JPEG2000, perceptual quality, structural information, structural similarity (SSIM)},
}

@InProceedings{WonRL2019,
  author    = {Changhee Won and Jongbin Ryu and Jongwoo Lim},
  title     = {{OmniMVS}: End-to-End Learning for Omnidirectional Stereo Matching},
  booktitle = ICCV,
  year      = {2019},
  pages     = {8986--8995},
  abstract  = {In this paper, we propose a novel end-to-end deep neural network model for omnidirectional depth estimation from a wide-baseline multi-view stereo setup. The images captured with ultra wide field-of-view (FOV) cameras on an omnidirectional rig are processed by the feature extraction module, and then the deep feature maps are warped onto the concentric spheres swept through all candidate depths using the calibrated camera parameters. The 3D encoder-decoder block takes the aligned feature volume to produce the omnidirectional depth estimate with regularization on uncertain regions utilizing the global context information. In addition, we present large-scale synthetic datasets for training and testing omnidirectional multi-view stereo algorithms. Our datasets consist of 11K ground-truth depth maps and 45K fisheye images in four orthogonal directions with various objects and environments. Experimental results show that the proposed method generates excellent results in both synthetic and real-world environments, and it outperforms the prior art and the omnidirectional versions of the state-of-the-art conventional stereo algorithms.},
  arxiv     = {1908.06257},
  doi       = {10.1109/ICCV.2019.00908},
  _url       = {http://cvlab.hanyang.ac.kr/project/omnistereo/},
}

@InProceedings{WonRL2019a,
  author    = {Changhee Won and Jongbin Ryu and Jongwoo Lim},
  title     = {{SweepNet}: Wide-baseline Omnidirectional Depth Estimation},
  booktitle = ICRA,
  year      = {2019},
  abstract  = {Omnidirectional depth sensing has its advantage over the conventional stereo systems since it enables us to recognize the objects of interest in all directions without any blind regions. In this paper, we propose a novel wide-baseline omnidirectional stereo algorithm which computes the dense depth estimate from the fisheye images using a deep convolutional neural network. The capture system consists of multiple cameras mounted on a wide-baseline rig with ultrawide field of view (FOV) lenses, and we present the calibration algorithm for the extrinsic parameters based on the bundle adjustment. Instead of estimating depth maps from multiple sets of rectified images and stitching them, our approach directly generates one dense omnidirectional depth map with full 360-degree coverage at the rig global coordinate system. To this end, the proposed neural network is designed to output the cost volume from the warped images in the sphere sweeping method, and the final depth map is estimated by taking the minimum cost indices of the aggregated cost volume by SGM. For training the deep neural network and testing the entire system, realistic synthetic urban datasets are rendered using Blender. The experiments using the synthetic and real-world datasets show that our algorithm outperforms the conventional depth estimation methods and generate highly accurate depth maps.},
  arxiv     = {1902.10904},
  doi       = {10.1109/ICRA.2019.8793823},
  _url       = {https://arxiv.org/abs/1902.10904},
}

@InProceedings{WonSCPL2020,
  author    = {Changhee Won and Hochang Seok and Zhaopeng Cui and Marc Pollefeys and Jongwoo Lim},
  title     = {{OmniSLAM}: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems},
  booktitle = ICRA,
  year      = {2020},
  pages     = {559--566},
  abstract  = {In this paper, we present an omnidirectional localization and dense mapping system for a wide-baseline multiview stereo setup with ultra-wide field-of-view (FOV) fisheye cameras, which has a 360 degrees coverage of stereo observations of the environment. For more practical and accurate reconstruction, we first introduce improved and light-weighted deep neural networks for the omnidirectional depth estimation, which are faster and more accurate than the existing networks. Second, we integrate our omnidirectional depth estimates into the visual odometry (VO) and add a loop closing module for global consistency. Using the estimated depth map, we reproject keypoints onto each other view, which leads to a better and more efficient feature matching process. Finally, we fuse the omnidirectional depth maps and the estimated rig poses into the truncated signed distance function (TSDF) volume to acquire a 3D map. We evaluate our method on synthetic datasets with ground-truth and real-world sequences of challenging environments, and the extensive experiments show that the proposed system generates excellent reconstruction results in both synthetic and real-world environments.},
  arxiv     = {2003.08056},
  doi       = {10.1109/ICRA40945.2020.9196695},
}

@InProceedings{ZhangLSC2019,
  author    = {Chao Zhang and Stephan Liwicki and William Smith and Roberto Cipolla},
  title     = {Orientation-Aware Semantic Segmentation on Icosahedron Spheres},
  booktitle = ICCV,
  year      = {2019},
  pages     = {3533--3541},
  _month     = oct,
  abstract  = {We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.},
  doi       = {10.1109/ICCV.2019.00363},
  _issn      = {1550-5499},
  _url       = {http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Orientation-Aware_Semantic_Segmentation_on_Icosahedron_Spheres_ICCV_2019_paper.html},
}

@Article{ZimmeBW2011a,
  author    = {Henning Zimmer and Andrés Bruhn and Joachim Weickert},
  title     = {Optic Flow in Harmony},
  journal   = IJCV,
  year      = {2011},
  volume    = {93},
  number    = {3},
  pages     = {368--388},
  _issn      = {0920-5691},
  abstract  = {Most variational optic flow approaches just consist of three constituents: a data term, a smoothness term and a smoothness weight. In this paper, we present an approach that harmonises these three components. We start by developing an advanced data term that is robust under outliers and varying illumination conditions. This is achieved by using constraint normalisation, and an HSV colour representation with higher order constancy assumptions and a separate robust penalisation. Our novel anisotropic smoothness is designed to work complementary to the data term. To this end, it incorporates directional information from the data constraints to enable a filling-in of information solely in the direction where the data term gives no information, yielding an optimal complementary smoothing behaviour. This strategy is applied in the spatial as well as in the spatio-temporal domain. Finally, we propose a simple method for automatically determining the optimal smoothness weight. This method bases on a novel concept that we call optimal prediction principle (OPP). It states that the flow field obtained with the optimal smoothness weight allows for the best prediction of the next frames in the image sequence. The benefits of our optic flow in harmony (OFH) approach are demonstrated by an extensive experimental validation and by a competitive performance at the widely used Middlebury optic flow benchmark.},
  doi       = {10.1007/s11263-011-0422-6},
  issue     = {3},
  keyword   = {Computer Science},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;author;false;editor;false;year;true;}
