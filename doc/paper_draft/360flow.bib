% Encoding: UTF-8

@String{AAAI           = {Proceedings of the Conference on Artificial Intelligence (AAAI)}}
@String{ACCV           = {ACCV}}
@String{ACCVW          = {Proceedings of ACCV Workshops}}
@String{AFRIGRAPH      = {Proceedings of the International Conference on Computer Graphics, Virtual Reality, Visualisation and Interaction in Africa (Afrigraph)}}
@String{APGV           = {Proceedings of the Symposium on Applied Perception in Graphics and Visualization}}
@String{apr            = {April}}
@String{aug            = {August}}
@String{BMVC           = {BMVC}}
@String{CAe            = {CAe}}
@String{CAG            = {Computers \& Graphics}}
@String{CGA            = {IEEE Comput. Graph. Appl.}}
@String{CGF            = {Comput. Graph. Forum}}
@String{CGI            = {Proceedings of Computer Graphics International (CGI)}}
@String{CHI            = {CHI}}
@String{CORL           = {Proceedings of the Conference on Robot Learning (CoRL)}}
@String{CVIU           = {Comput. Vision Image Understanding}}
@String{CVM            = {Computational Visual Media}}
@String{CVMP           = {CVMP}}
@String{CVPR           = {CVPR}}
@String{CVPRW          = {CVPR Workshops}}
@String{DAGM           = {Pattern Recognition (Proceedings of the DAGM Symposium)}}
@String{dec            = {December}}
@String{ECCV           = {ECCV}}
@String{ECCVW          = {ECCV Workshops}}
@String{EGSR           = {Proceedings of the Eurographics Symposium on Rendering}}
@String{EGWR           = {Proceedings of the Eurographics Workshop on Rendering}}
@String{ETRA           = {ETRA}}
@String{feb            = {February}}
@String{FG             = {Proceedings of the International Conference on Automatic Face and Gesture Recognition (FG)}}
@String{GCPR           = {Pattern Recognition (Proceedings of the German Conference on Pattern Recognition)}}
@String{GI             = {Graphics Interface}}
@String{GRAPHITE       = {Proceedings of the International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia (GRAPHITE)}}
@String{HPG            = {Proceedings of High Performance Graphics (HPG)}}
@String{I3D            = {Proceedings of the Symposium on Interactive 3D Graphics and Games (I3D)}}
@String{ICASSP         = {ICASSP}}
@String{ICCP           = {ICCP}}
@String{ICCV           = {ICCV}}
@String{ICCVW          = {ICCV Workshops}}
@String{ICIP           = {ICIP}}
@String{ICLR           = {ICLR}}
@String{ICME           = {ICME}}
@String{ICML           = {Proceedings of the International Conference on Machine Learning (ICML)}}
@String{ICPR           = {ICPR}}
@String{ICRA           = {ICRA}}
@String{IJCV           = {IJCV}}
@String{IROS           = {IROS}}
@String{ISM            = {Proceedings of the International Symposium on Multimedia (ISM)}}
@String{ISMAR          = {ISMAR}}
@String{jan            = {January}}
@String{JCGT           = {Journal of Computer Graphics Techniques (JCGT)}}
@String{jul            = {July}}
@String{jun            = {June}}
@String{LNCS           = {Lecture Notes in Computer Science}}
@String{mar            = {March}}
@String{may            = {May}}
@String{MULTIMEDIA     = {Proceedings of the International Conference on Multimedia}}
@String{NeurIPS        = {NeurIPS}}
@String{NeurIPSW       = {NeurIPS Workshops}}
@String{NIPS           = {NIPS}}
@String{NIPSW          = {NIPS Workshops}}
@String{nov            = {November}}
@String{NPAR           = {Proceedings of the International Symposium on Non-Photorealistic Animation and Rendering (NPAR)}}
@String{oct            = {October}}
@String{PAMI           = {TPAMI}}
@String{Proc3DIM       = {Proceedings of the International Workshop on 3D Digital Imaging and Modeling (3DIM)}}
@String{Proc3DIMPVT    = {Proceedings of the International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT)}}
@String{Proc3DPVT      = {Proceedings of the International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT)}}
@String{Proc3DTV       = {Proceedings of the 3DTV Conference}}
@String{Proc3DV        = {3DV}}
@String{PROCAMS        = {Proceedings of the International Workshop on Projector-Camera Systems (PROCAMS)}}
@String{ProcCGIT       = {Proceedings of the ACM on Computer Graphics and Interactive Techniques}}
@String{ProcCVM        = {Proceedings of Computational Visual Media (CVM)}}
@String{ProcEG         = {Comput. Graph. Forum}}
@String{ProcEGSR       = {Comput. Graph. Forum}}
@String{ProcPG         = {Comput. Graph. Forum}}
@String{ProcSGP        = {Comput. Graph. Forum}}
@String{ProcSIGASIA    = {ACM Trans. Graph.}}
@String{ProcSIGGRAPH   = {ACM Trans. Graph.}}
@String{ProcSIGGRAPHCG = {Computer Graphics (Proceedings of SIGGRAPH)}}
@String{ProcSPIE       = {Proceedings of SPIE}}
@String{RSS            = {Proceedings of Robotics: Science and Systems (RSS)}}
@String{SAP            = {Proceedings of the Symposium on Applied Perception (SAP)}}
@String{SBIM           = {Proceedings of the Symposium on Sketch-Based Interfaces and Modeling (SBIM)}}
@String{SCA            = {Proceedings of the Symposium on Computer Animation (SCA)}}
@String{sep            = {September}}
@String{SIGGRAPH       = {SIGGRAPH}}
@String{TAP            = {ACM Trans. Appl. Percept.}}
@String{TCSVT          = {IEEE Transactions on Circuits and Systems for Video Technology}}
@String{TIP            = {IEEE Transactions on Image Processing}}
@String{TOG            = {ACM Trans. Graph.}}
@String{TPCG           = {Proceedings of Theory and Practice of Computer Graphics (TPCG)}}
@String{TVCG           = {TVCG}}
@String{UIST           = {UIST}}
@String{VMV            = {Proceedings of the Vision, Modeling, and Visualization Workshop (VMV)}}
@String{VR             = {IEEE VR}}
@String{VR3DUI         = {IEEE VR}}
@String{VRST           = {VRST}}
@String{VRW            = {Proceedings of IEEE Virtual Reality Workshops}}
@String{WACV           = {WACV}}
@String{WACVW          = {Proceedings of WACV Workshops}}
@String{WSBIM          = {Proceedings of the Workshop on Sketch-Based Interfaces and Modeling (SBIM)}}
@String{WSCG           = {Proceedings of the International Conference in Central Europe on Computer Graphics, Visualisation and Computer Vision (WSCG)}}

@InProceedings{AlbanZDGSAZD2021,
  author        = {Georgios Albanis and Nikolaos Zioulis and Petros Drakoulis and Vasileios Gkitsas and Vladimiros Sterzentsenko and Federico Alvarez and Dimitrios Zarpalas and Petros Daras},
  title         = {{Pano3D}: A Holistic Benchmark and a Solid Baseline for 360° Depth Estimation},
  booktitle     = CVPRW,
  year          = {2021},
  pages         = {3727--3737},
  _month         = jun,
  abstract      = {Pano3D is a new benchmark for depth estimation from spherical panoramas. It aims to assess performance across all depth estimation traits, the primary direct depth estimation performance targeting precision and accuracy, and also the secondary traits, boundary preservation, and smoothness. Moreover, Pano3D moves beyond typical intro-dataset evaluation to inter-dataset performance assessment. By disentangling the capacity to generalize in unseen data into different test splits, Pano3D represents a holistic benchmark for 360 depth estimation. We use it as a basis for an extended analysis seeking to offer insights into classical choices for depth estimation. This results in a solid baseline for panoramic depth that follow-up works can build upon to steer future progress},
  _url           = {https://vcl3d.github.io/Pano3D/},
}

@InProceedings{AleotPM2021,
  author    = {Filippo Aleotti and Matteo Poggi and Stefano Mattoccia},
  title     = {Learning optical flow from still images},
  booktitle = CVPR,
  year      = {2021},
  abstract  = {This paper deals with the scarcity of data for training optical flow networks, highlighting the limitations of existing sources such as labeled synthetic datasets or unlabeled real videos. Specifically, we introduce a framework to generate accurate ground-truth optical flow annotations quickly and in large amounts from any readily available single real picture. Given an image, we use an off-the-shelf monocular depth estimation network to build a plausible point cloud for the observed scene. Then, we virtually move the camera in the reconstructed environment with known motion vectors and rotation angles, allowing us to synthesize both a novel view and the corresponding optical flow field connecting each pixel in the input image to the one in the new frame. When trained with our data, state-of-the-art optical flow networks achieve superior generalization to unseen real data compared to the same models trained either on annotated synthetic datasets or unlabeled videos, and better specialization if combined with synthetic images.},
  arxiv     = {2104.03965},
  _url       = {https://arxiv.org/abs/2104.03965},
}

@Unpublished{ArmenSZS2017,
  author    = {Iro Armeni and Sasha Sax and Amir R. Zamir and Silvio Savarese},
  title     = {Joint {{2D}-3D}-Semantic Data for Indoor Scene Understanding},
  note      = {arXiv:\href{https://arxiv.org/abs/1702.01105}{1702.01105}},
  year      = {2017},
  abstract  = {We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360° equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: http://3Dsemantics.stanford.edu/},
  arxiv     = {1702.01105},
  _url       = {https://github.com/alexsax/2D-3D-Semantics},
}

@InProceedings{ArmenSZJBFS2016,
  author    = {Iro Armeni and Ozan Sener and Amir R. Zamir and Helen Jiang and Ioannis Brilakis and Martin Fischer and Silvio Savarese},
  title     = {{3D} Semantic Parsing of Large-Scale Indoor Spaces},
  booktitle = CVPR,
  year      = {2016},
  _month     = jun,
  abstract  = {In this paper, we propose a method for semantic parsing the 3D point cloud of an entire building using a hierarchical approach: first, the raw data is parsed into semantically meaningful spaces (e.g. rooms, etc) that are aligned into a canonical reference coordinate system. Second, the spaces are parsed into their structural and building elements (e.g. walls, columns, etc). Performing these with a strong notation of global 3D space is the backbone of our method. The alignment in the first step injects strong 3D priors from the canonical coordinate system into the second step for dis- covering elements. This allows diverse challenging scenarios as man-made indoor spaces often show recurrent geo- metric patterns while the appearance features can change drastically. We also argue that identification of structural elements in indoor spaces is essentially a detection problem, rather than segmentation which is commonly used. We evaluated our method on a new dataset of several buildings with a covered area of over 6,000 m2 and over 215 million points, demonstrating robust results readily useful for practical applications.},
}

@InProceedings{ArtizZAD2020,
  author    = {Charles-Olivier Artizzu and Haozhou Zhang and Guillaume Allibert and Cédric Demonceaux},
  title     = {{OmniFlowNet}: a Perspective Neural Network Adaptation for Optical Flow Estimation in Omnidirectional Images},
  booktitle = ICPR,
  year      = {2020},
  abstract  = {Spherical cameras and the latest image processing techniques open up new horizons. In particular, methods based on Convolutional Neural Networks (CNNs) now give excellent results for optical flow estimation on perspective images. However, these approaches are highly dependent on their architectures and training datasets. This paper proposes to benefit from years of improvement in perspective images optical flow estimation and to apply it to omnidirectional ones without training on new datasets. Our network, OmniFlowNet, is built on a CNN specialized in perspective images. Its convolution operation is adapted to be consistent with the equirectangular projection. Tested on spherical datasets created with Blender 1 and several equirectangular videos realized from real indoor and outdoor scenes, OmniFlowNet shows better performance than its original network without extra training.},
  doi       = {10.1109/ICPR48806.2021.9412745},
  _url       = {https://hal.archives-ouvertes.fr/hal-02968191},
}

@Article{BakerSLRBS2011,
  author    = {Simon Baker and Daniel Scharstein and J. Lewis and Stefan Roth and Michael Black and Richard Szeliski},
  title     = {A Database and Evaluation Methodology for Optical Flow},
  journal   = IJCV,
  year      = {2011},
  volume    = {92},
  number    = {1},
  pages     = {1--31},
  _issn      = {0920-5691},
  note      = {10.1007/s11263-010-0390-2},
  abstract  = {The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1) sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2) realistic synthetic sequences, (3) high frame-rate video used to study interpolation error, and (4) modified stereo sequences of static scenes. In addition to the average angular error used by Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/. Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them.},
  doi       = {10.1007/s11263-010-0390-2},
  issue     = {1},
}

@Article{BerteYLR2020,
  author    = {Tobias Bertel and Mingze Yuan and Reuben Lindroos and Christian Richardt},
  title     = {{OmniPhotos}: Casual 360° {VR} Photography},
  journal   = ProcSIGASIA,
  year      = {2020},
  volume    = {39},
  number    = {6},
  pages     = {267:1--12},
  _month     = dec,
  _issn      = {0730-0301},
  abstract  = {Virtual reality headsets are becoming increasingly popular, yet it remains difficult for casual users to capture immersive 360° VR panoramas. State-of-the-art approaches require capture times of usually far more than a minute and are often limited in their supported range of head motion. We introduce OmniPhotos, a novel approach for quickly and casually capturing high-quality 360° panoramas with motion parallax. Our approach requires a single sweep with a consumer 360° video camera as input, which takes less than 3 seconds to capture with a rotating selfie stick or 10 seconds handheld. This is the fastest capture time for any VR photography approach supporting motion parallax by an order of magnitude. We improve the visual rendering quality of our OmniPhotos by alleviating vertical distortion using a novel deformable proxy geometry, which we fit to a sparse 3D reconstruction of captured scenes. In addition, the 360° input views significantly expand the available viewing area, and thus the range of motion, compared to previous approaches. We have captured more than 50 OmniPhotos and show video results for a large variety of scenes. We will make our code available.},
  doi       = {10.1145/3414685.3417770},
  _url       = {https://richardt.name/omniphotos/},
}

@InProceedings{BhandZY2020,
  author    = {Keshav Bhandari and Ziliang Zong and Yan Yan},
  title     = {Revisiting Optical Flow Estimation in 360 Videos},
  booktitle = ICPR,
  year      = {2020},
  abstract  = {Nowadays 360 video analysis has become a significant research topic in the field since the appearance of high-quality and low-cost 360 wearable devices. In this paper, we propose a novel LiteFlowNet360 architecture for 360 videos optical flow estimation. We design LiteFlowNet360 as a domain adaptation framework from perspective video domain to 360 video domain. We adapt it from simple kernel transformation techniques inspired by Kernel Transformer Network (KTN) to cope with inherent distortion in 360 videos caused by the sphere-to-plane projection. First, we apply an incremental transformation of convolution layers in feature pyramid network and show that further transformation in inference and regularization layers are not important, hence reducing the network growth in terms of size and computation cost. Second, we refine the network by training with augmented data in a supervised manner. We perform data augmentation by projecting the images in a sphere and re-projecting to a plane. Third, we train LiteFlowNet360 in a self-supervised manner using target domain 360 videos. Experimental results show the promising results of 360 video optical flow estimation using the proposed novel architecture.},
  arxiv     = {2010.08045},
  doi       = {10.1109/ICPR48806.2021.9412035},
  _url       = {https://arxiv.org/abs/2010.08045},
}

@Article{BlackA1996,
  author    = {Michael J. Black and P. Anandan},
  title     = {The robust estimation of multiple motions: parametric and piecewise-smooth flow fields},
  journal   = CVIU,
  year      = {1996},
  volume    = {63},
  number    = {1},
  pages     = {75--104},
  _issn      = {1077-3142},
  abstract  = {Most approaches for estimating optical flow assume that, within a finite image region, only a single motion is present. This single motion assumption is violated in common situations involving transparency, depth discontinuities, independently moving objects, shadows, and specular reflections. To robustly estimate optical flow, the single motion assumption must be relaxed. This paper presents a framework based on robust estimation that addresses violations of the brightness constancy and spatial smoothness assumptions caused by multiple motions. We show how the robust estimation framework can be applied to standard formulations of the optical flow problem thus reducing their sensitivity to violations of their underlying assumptions. The approach has been applied to three standard techniques for recovering optical flow: area-based regression, correlation, and regularization with motion discontinuities. This paper focuses on the recovery of multiple parametric motion models within a region, as well as the recovery of piecewise-smooth flow fields, and provides examples with natural and synthetic image sequences.},
  doi       = {10.1006/cviu.1996.0006},
}

@InProceedings{BlackA1991,
  author      = {M. J. Black and P. Anandan},
  title       = {Robust dynamic motion estimation over time},
  booktitle   = CVPR,
  year        = {1991},
  pages       = {296--302},
  abstract    = {A novel approach to incrementally estimating visual motion over a sequence of images is presented. The authors start by formulating constraints on image motion to account for the possibility of multiple motions. This is achieved by exploiting the notions of weak continuity and robust statistics in the formulation of a minimization problem. The resulting objective function is non-convex. Traditional stochastic relaxation techniques for minimizing such functions prove inappropriate for the task. A highly parallel incremental stochastic minimization algorithm is presented which has a number of advantages over previous approaches. The incremental nature of the scheme makes it dynamic and permits the detection of occlusion and disocclusion boundaries.},
  citeseerurl = {http://citeseer.ist.psu.edu/black91robust.html},
  doi         = {10.1109/CVPR.1991.139705},
}

@InProceedings{BroxBPW2004,
  author    = {Thomas Brox and Andrés Bruhn and Nils Papenberg and Joachim Weickert},
  title     = {High Accuracy Optical Flow Estimation Based on a Theory for Warping},
  booktitle = ECCV,
  year      = {2004},
  volume    = {3024},
  _series    = LNCS,
  pages     = {25--36},
  _month     = may,
  abstract  = {We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise.},
  doi       = {10.1007/978-3-540-24673-2_3},
}

@InProceedings{ButleWSB2012,
  author    = {Daniel J. Butler and Jonas Wulff and Garrett B. Stanley and Michael J. Black},
  title     = {A Naturalistic Open Source Movie for Optical Flow Evaluation},
  booktitle = ECCV,
  year      = {2012},
  _editor    = {Andrew Fitzgibbon and Svetlana Lazebnik and Pietro Perona and Yoichi Sato and Cordelia Schmid},
  volume    = {7577},
  _series    = LNCS,
  pages     = {611--625},
  abstract  = {Ground truth optical flow is difficult to measure in real scenes with natural motion. As a result, optical flow data sets are restricted in terms of size, complexity, and diversity, making optical flow algorithms difficult to train and test on realistic data. We introduce a new optical flow data set derived from the open source 3D animated short film Sintel. This data set has important features not present in the popular Middlebury flow evaluation: long sequences, large motions, specular reflections, motion blur, defocus blur, and atmospheric effects. Because the graphics data that generated the movie is open source, we are able to render scenes under conditions of varying complexity to evaluate where existing flow algorithms fail. We evaluate several recent optical flow algorithms and find that current highly-ranked methods on the Middlebury evaluation have difficulty with this more complex data set suggesting further research on optical flow estimation is needed. To validate the use of synthetic data, we compare the image- and flow-statistics of Sintel to those of real films and videos and show that they are similar. The data set, metrics, and evaluation website are publicly available.},
  doi       = {10.1007/978-3-642-33783-3_44},
  _isbn      = {978-3-642-33782-6},
  _url       = {http://sintel.is.tue.mpg.de/},
}

@InProceedings{ChangDFHNSSZZ2017,
  author    = {Angel Chang and Angela Dai and Thomas Funkhouser and Maciej Halber and Matthias Nießner and Manolis Savva and Shuran Song and Andy Zeng and Yinda Zhang},
  title     = {{Matterport3D}: Learning from {RGB-D} Data in Indoor Environments},
  booktitle = Proc3DV,
  year      = {2017},
  pages     = {667--676},
  abstract  = {Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.},
  arxiv     = {1709.06158},
  doi       = {10.1109/3DV.2017.00081},
  _url       = {https://github.com/niessner/Matterport},
}

@Article{ChenLFLCG2021,
  author    = {Hong-Xiang Chen and Kunhong Li and Zhiheng Fu and Mengyi Liu and Zonghao Chen and Yulan Guo},
  title     = {Distortion-Aware Monocular Depth Estimation for Omnidirectional Images},
  journal   = {IEEE Signal Processing Letters},
  year      = {2021},
  volume    = {28},
  pages     = {334--338},
  _month     = jan,
  _issn      = {1558-2361},
  abstract  = {Image distortion is a main challenge for tasks on panoramas. In this work, we propose a Distortion-Aware Monocular Omnidirectional (DAMO) network to estimate dense depth maps from indoor panoramas. First, we introduce a distortion-aware module to extract semantic features from omnidirectional images. Specifically, we exploit deformable convolution to adjust its sampling grids to geometric distortions on panoramas. We also utilize a strip pooling module to sample against horizontal distortion introduced by inverse gnomonic projection. Second, we introduce a plug-and-play spherical-aware weight matrix for our loss function to handle the uneven distribution of areas projected from a sphere. Experiments on the 360D dataset show that the proposed method can effectively extract semantic features from distorted panoramas and alleviate the supervision bias caused by distortion. It achieves the state-of-the-art performance on the 360D dataset with high efficiency.},
  arxiv     = {2010.08942},
  doi       = {10.1109/LSP.2021.3050712},
}

@InProceedings{ChengCDWLS2018,
  author    = {Hsien-Tzu Cheng and Chun-Hung Chao and Jin-Dong Dong and Hao-Kai Wen and Tyng-Luh Liu and Min Sun},
  title     = {Cube Padding for Weakly-Supervised Saliency Prediction in 360° Videos},
  booktitle = CVPR,
  year      = {2018},
  pages     = {1420--1429},
  _month     = jun,
  abstract  = {Automatic saliency prediction in 360° videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) unsupervisedly trained and (2) tailor-made for 360° viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360° sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360° view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, PC introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360° video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms all baseline methods in both speed and quality.},
  doi       = {10.1109/CVPR.2018.00154},
  _issn      = {1063-6919},
  _url       = {http://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_Cube_Padding_for_CVPR_2018_paper.html},
}

@InProceedings{CobbWMMPdM2020,
  author    = {Oliver J. Cobb and Christopher G. R. Wallis and Augustine N. Mavor-Parker and Augustin Marignier and Matthew A. Price and Mayeul d'Avezac and Jason D. McEwen},
  title     = {Efficient Generalized Spherical {CNNs}},
  booktitle = ICLR,
  year      = {2020},
  abstract  = {Many problems across computer vision and the natural sciences require the analysis of spherical data, for which representations may be learned efficiently by encoding equivariance to rotational symmetries. We present a generalized spherical CNN framework that encompasses various existing approaches and allows them to be leveraged alongside each other. The only existing non-linear spherical CNN layer that is strictly equivariant has complexity $\mathcalO(C^2L^5)$, where $C$ is a measure of representational capacity and $L$ the spherical harmonic bandlimit. Such a high computational cost often prohibits the use of strictly equivariant spherical CNNs. We develop two new strictly equivariant layers with reduced complexity $\mathcalO(CL^4)$ and $\mathcalO(CL^3 \log L)$, making larger, more expressive models computationally feasible. Moreover, we adopt efficient sampling theory to achieve further computational savings. We show that these developments allow the construction of more expressive hybrid models that achieve state-of-the-art accuracy and parameter efficiency on spherical benchmark problems.},
  arxiv     = {2010.11661},
  _url       = {https://arxiv.org/abs/2010.11661},
}

@InProceedings{CohenGKW2018,
  author    = {Taco S. Cohen and Mario Geiger and Jonas Koehler and Max Welling},
  title     = {Spherical {CNNs}},
  booktitle = ICLR,
  year      = {2018},
  abstract  = {Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective.

In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.},
  arxiv     = {1801.10130},
  _url       = {https://arxiv.org/abs/1801.10130},
}

@InProceedings{CoorsCG2018,
  author    = {Benjamin Coors and Alexandru Paul Condurache and Andreas Geiger},
  title     = {{SphereNet}: Learning Spherical Representations for Detection and Classification in Omnidirectional Images},
  booktitle = ECCV,
  year      = {2018},
  pages     = {518--533},
  _month     = sep,
  abstract  = {Omnidirectional cameras offer great benefits over classical cameras wherever a wide field of view is essential, such as in virtual reality applications or in autonomous robots. Unfortunately, standard convolutional neural networks are not well suited for this scenario as the natural projection surface is a sphere which cannot be unwrapped to a plane without introducing significant distortions, particularly in the polar regions. In this work, we present SphereNet, a novel deep learning framework which encodes invariance against such distortions explicitly into convolutional neural networks. Towards this goal, SphereNet adapts the sampling locations of the convolutional filters, effectively reversing distortions, and wraps the filters around the sphere. By building on regular convolutions, SphereNet enables the transfer of existing perspective convolutional neural network models to the omnidirectional case. We demonstrate the effectiveness of our method on the tasks of image classification and object detection, exploiting two newly created semi-synthetic and real-world omnidirectional datasets.},
  doi       = {10.1007/978-3-030-01240-3_32},
  _isbn      = {978-3-030-01240-3},
  _url       = {http://openaccess.thecvf.com/content_ECCV_2018/html/Benjamin_Coors_SphereNet_Learning_Spherical_ECCV_2018_paper.html},
}

@InProceedings{DosovFIHHGSCB2015,
  author    = {Alexey Dosovitskiy and Philipp Fischer and Eddy Ilg and Philip Hausser and Caner Hazirbas and Vladimir Golkov and Patrick van der Smagt and Daniel Cremers and Thomas Brox},
  title     = {{FlowNet}: Learning Optical Flow With Convolutional Networks},
  booktitle = ICCV,
  year      = {2015},
  abstract  = {Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.},
  doi       = {10.1109/ICCV.2015.316},
}

@InProceedings{EderMG2019,
  author    = {Marc Eder and Pierre Moulon and Li Guan},
  title     = {Pano Popups: Indoor {3D} Reconstruction with a Plane-Aware Network},
  booktitle = Proc3DV,
  year      = {2019},
  pages     = {76--84},
  abstract  = {In this work we present a method to train a plane-aware convolutional neural network for dense depth and surface normal estimation as well as plane boundaries from a single indoor 360 image. Using our proposed loss function, our network outperforms existing methods for single-view, indoor, omnidirectional depth estimation and provides an initial benchmark for surface normal prediction from 360 images. Our improvements are due to the use of a novel plane-aware loss that leverages principal curvature as an indicator of planar boundaries. We also show that including geodesic coordinate maps as network priors provides a significant boost in surface normal prediction accuracy. Finally, we demonstrate how we can combine our network's outputs to generate high quality 3D "pop-up" models of indoor scenes.},
  arxiv     = {1907.00939},
  doi       = {10.1109/3DV.2019.00018},
  _issn      = {2475-7888},
  _url       = {https://arxiv.org/abs/1907.00939},
}

@Unpublished{EderPVBF2019,
  author    = {Marc Eder and True Price and Thanh Vu and Akash Bapat and Jan-Michael Frahm},
  title     = {Mapped Convolutions},
  note      = {arXiv:\href{https://arxiv.org/abs/1906.11096}{1906.11096}},
  year      = {2019},
  abstract  = {We present a versatile formulation of the convolution operation that we term a "mapped convolution." The standard convolution operation implicitly samples the pixel grid and computes a weighted sum. Our mapped convolution decouples these two components, freeing the operation from the confines of the image grid and allowing the kernel to process any type of structured data. As a test case, we demonstrate its use by applying it to dense inference on spherical data. We perform an in-depth study of existing spherical image convolution methods and propose an improved sampling method for equirectangular images. Then, we discuss the impact of data discretization when deriving a sampling function, highlighting drawbacks of the cube map representation for spherical data. Finally, we illustrate how mapped convolutions enable us to convolve directly on a mesh by projecting the spherical image onto a geodesic grid and training on the textured mesh. This method exceeds the state of the art for spherical depth estimation by nearly 17%. Our findings suggest that mapped convolutions can be instrumental in expanding the application scope of convolutional neural networks.},
  arxiv     = {1906.11096},
  _url       = {https://github.com/meder411/MappedConvolutions},
}

@InProceedings{EderSLF2020,
  author    = {Marc Eder and Mykhailo Shvets and John Lim and Jan-Michael Frahm},
  title     = {Tangent Images for Mitigating Spherical Distortion},
  booktitle = CVPR,
  year      = {2020},
  abstract  = {In this work, we propose "tangent images," a spherical image representation that facilitates transferable and scalable 360° computer vision. Inspired by techniques in cartography and computer graphics, we render a spherical image to a set of distortion-mitigated, locally-planar image grids tangent to a subdivided icosahedron. By varying the resolution of these grids independently of the subdivision level, we can effectively represent high resolution spherical images while still benefiting from the low-distortion icosahedral spherical approximation. We show that training standard convolutional neural networks on tangent images compares favorably to the many specialized spherical convolutional kernels that have been developed, while also allowing us to scale training to significantly higher spherical resolutions. Furthermore, because we do not require specialized kernels, we show that we can transfer networks trained on perspective images to spherical data without fine-tuning and with limited performance drop-off. Finally, we demonstrate that tangent images can be used to improve the quality of sparse feature detection on spherical images, illustrating its usefulness for traditional computer vision tasks like structure-from-motion and SLAM.},
  arxiv     = {1912.09390},
  doi       = {10.1109/CVPR42600.2020.01244},
  _url       = {https://github.com/meder411/Tangent-Images},
}

@InProceedings{EstevAMD2018,
  author    = {Carlos Esteves and Christine Allen-Blanchette and Ameesh Makadia and Kostas Daniilidis},
  title     = {Learning {SO(3)} Equivariant Representations with Spherical {CNNs}},
  booktitle = ECCV,
  year      = {2018},
  pages     = {52--68},
  _month     = sep,
  abstract  = {We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.},
  arxiv     = {1711.06721},
  doi       = {10.1007/978-3-030-01261-8_4},
  _isbn      = {978-3-030-01261-8},
  _url       = {https://github.com/daniilidis-group/spherical-cnn},
}

@Article{FernaFPDCG2020,
  author    = {Clara Fernandez-Labrador and Jose M. Facil and Alejandro Perez-Yus and Cédric Demonceaux and Javier Civera and Jose J. Guerrero},
  title     = {Corners for Layout: End-to-End Layout Recovery from 360 Images},
  journal   = {IEEE Robotics and Automation Letters},
  year      = {2020},
  volume    = {5},
  number    = {2},
  pages     = {1255--1262},
  _month     = apr,
  _issn      = {2377-3766},
  abstract  = {The problem of 3D layout recovery in indoor scenes has been a core research topic for over a decade. However, there are still several major challenges that remain unsolved. Among the most relevant ones, a major part of the state-of-the-art methods make implicit or explicit assumptions on the scenes -e.g. box-shaped or Manhattan layouts. Also, current methods are computationally expensive and not suitable for real-time applications like robot navigation and AR/YR. In this work we present CFL (Corners for Layout), the first end-to-end model that predicts layout corners for 3D layout recovery on 360° images. Our experimental results show that we outperform the state of the art, making less assumptions on the scene than other works, and with lower cost. We also show that our model generalizes better to camera position variations than conventional approaches by using EquiConvs, a convolution applied directly on the spherical projection and hence invariant to the equirectangular distortions.},
  arxiv     = {1903.08094},
  doi       = {10.1109/LRA.2020.2967274},
  _url       = {https://cfernandezlab.github.io/CFL/},
}

@InProceedings{GkitsZAZD2020,
  author    = {Vasileios Gkitsas and Nikolaos Zioulis and Federico Alvarez and Dimitrios Zarpalas and Petros Daras},
  title     = {Deep Lighting Environment Map Estimation from Spherical Panoramas},
  booktitle = CVPRW,
  year      = {2020},
  abstract  = {Estimating a scene's lighting is a very important task when compositing synthetic content within real environments, with applications in mixed reality and post-production. In this work we present a data-driven model that estimates an HDR lighting environment map from a single LDR monocular spherical panorama. In addition to being a challenging and ill-posed problem, the lighting estimation task also suffers from a lack of facile illumination ground truth data, a fact that hinders the applicability of data-driven methods. We approach this problem differently, exploiting the availability of surface geometry to employ image-based relighting as a data generator and supervision mechanism. This relies on a global Lambertian assumption that helps us overcome issues related to pre-baked lighting. We relight our training data and complement the model's supervision with a photometric loss, enabled by a differentiable image-based relighting technique. Finally, since we predict spherical spectral coefficients, we show that by imposing a distribution prior on the predicted coefficients, we can greatly boost performance. Code and models available at https://vcl3d.github.io/DeepPanoramaLighting
.},
  arxiv     = {2005.08000},
  doi       = {10.1109/CVPRW50498.2020.00328},
  _url       = {https://vcl3d.github.io/DeepPanoramaLighting/},
}

@InProceedings{GodarAB2017,
  author    = {Clément Godard and Oisin Mac Aodha and Gabriel J. Brostow},
  title     = {Unsupervised Monocular Depth Estimation with Left-Right Consistency},
  booktitle = CVPR,
  year      = {2017},
  pages     = {6602--6611},
  _month     = jul,
  abstract  = {Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage.

We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Exploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.},
  arxiv     = {1609.03677},
  doi       = {10.1109/CVPR.2017.699},
  _issn      = {1063-6919},
  _url       = {http://visual.cs.ucl.ac.uk/pubs/monoDepth/},
}

@InProceedings{HannuWH2019,
  author    = {Hannuksela, Miska M. and Wang, Ye-Kui and Hourunranta, Ari},
  title     = {An Overview of the OMAF Standard for 360° Video},
  booktitle = {Data Compression Conference (DCC)},
  year      = {2019},
  pages     = {418--427},
  abstract  = {Omnidirectional MediA Format (OMAF) is arguably the first virtual reality (VR) system standard, recently developed by the Moving Picture Experts Group (MPEG). OMAF defines a media format that enables omnidirectional media applications, focusing on 360° video, images, and audio, as well as the associated timed text, supporting three degrees of freedom (3DOF). This paper gives an overview of the first edition of the OMAF standard.},
  doi       = {10.1109/DCC.2019.00050},
  _issn      = {2375-0359},
}

@Article{HornS1981,
  author    = {Berthold K. P. Horn and Brian G. Schunck},
  title     = {Determining optical flow},
  journal   = {Artificial Intelligence},
  year      = {1981},
  volume    = {17},
  pages     = {185--203},
  _month     = aug,
  _issn      = {0004-3702},
  abstract  = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.},
  doi       = {10.1016/0004-3702(81)90024-2},
}

@InProceedings{HuangCCJ2017,
  author    = {Jingwei Huang and Zhili Chen and Duygu Ceylan and Hailin Jin},
  title     = {6-{DOF VR} videos with a single 360-camera},
  booktitle = VR,
  year      = {2017},
  pages     = {37--44},
  _month     = mar,
  abstract  = {Recent breakthroughs in consumer level virtual reality (VR) headsets are creating a growing user-base in demand for immersive, full 3D VR experiences. While monoscopic 360-videos are perhaps the most prevalent type of content for VR headsets, they lack 3D information and thus cannot be viewed with full 6 degree-of-freedom (DOF). We present an approach that addresses this limitation via a novel warping algorithm that can synthesize new views both with rotational and translational motion of the viewpoint. This enables the ability to perform VR playback of input monoscopic 360-videos files in full stereo with full 6-DOF of head motion. Our method synthesizes novel views for each eye in accordance with the 6-DOF motion of the headset. Our solution tailors standard structure-from-motion and dense reconstruction algorithms to work accurately for 360-videos and is optimized for GPUs to achieve VR frame rates (>120 fps). We demonstrate the effectiveness our approach on a variety of videos with interesting content.},
  doi       = {10.1109/VR.2017.7892229},
}

@InProceedings{HuiTC2018,
  author    = {Tak-Wai Hui and Xiaoou Tang and Change Loy, Chen},
  title     = {{LiteFlowNet}: A Lightweight Convolutional Neural Network for Optical Flow Estimation},
  booktitle = CVPR,
  year      = {2018},
  pages     = {8981--8989},
  _month     = jun,
  abstract  = {FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that attains performance on par with FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet.},
  doi       = {10.1109/CVPR.2018.00936},
  _url       = {http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/},
}

@Article{HuiTL2021,
  author    = {Tak-Wai Hui and Xiaoou Tang and Chen Change Loy},
  title     = {A Lightweight Optical Flow {CNN} - Revisiting Data Fidelity and Regularization},
  journal   = PAMI,
  year      = {2021},
  volume    = {preprints},
  _issn      = {1939-3539},
  abstract  = {Over four decades, the majority addresses the problem of optical flow estimation using variational methods. With the advance of machine learning, some recent works have attempted to address the problem using convolutional neural network (CNN) and have showed promising results. FlowNet2 [1], the state-of-the-art CNN, requires over 160M parameters to achieve accurate flow estimation. Our LiteFlowNet2 outperforms FlowNet2 on Sintel and KITTI benchmarks, while being 25.3 times smaller in the model size and 3.1 times faster in the running speed. LiteFlowNet2 is built on the foundation laid by conventional methods and resembles the corresponding roles as data fidelity and regularization in variational methods. We compute optical flow in a spatial-pyramid formulation as SPyNet [2] but through a novel lightweight cascaded flow inference. It provides high flow estimation accuracy through early correction with seamless incorporation of descriptor matching. Flow regularization is used to ameliorate the issue of outliers and vague flow boundaries through feature-driven local convolutions. Our network also owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2 and SPyNet. Comparing to LiteFlowNet [3], LiteFlowNet2 improves the optical flow accuracy on Sintel Clean by 23.3%, Sintel Final by 12.8%, KITTI 2012 by 19.6%, and KITTI 2015 by 18.8%, while being 2.2 times faster. Our network protocol and trained models are made publicly available on https://github.com/twhui/LiteFlowNet2.},
  doi       = {10.1109/TPAMI.2020.2976928},
}

@InProceedings{IlgMSKDB2017,
  author    = {Eddy Ilg and Nikolaus Mayer and Tonmoy Saikia and Margret Keuper and Alexey Dosovitskiy and Thomas Brox},
  title     = {{FlowNet} 2.0: Evolution of Optical Flow Estimation with Deep Networks},
  booktitle = CVPR,
  year      = {2017},
  abstract  = {The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a sub-network specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.},
  arxiv     = {1612.01925},
  doi       = {10.1109/CVPR.2017.179},
  _url       = {https://lmb.informatik.uni-freiburg.de/Publications/2017/IMKDB17/},
}

@InProceedings{ImHRJCK2016,
  author    = {Sunghoon Im and Hyowon Ha and François Rameau and Hae-Gon Jeon and Gyeongmin Choe and In So Kweon},
  title     = {All-around Depth from Small Motion with A Spherical Panoramic Camera},
  booktitle = ECCV,
  year      = {2016},
  _editor    = {Bastian Leibe and Jiri Matas and Nicu Sebe and Max Welling},
  abstract  = {With the growing use of head-mounted displays for virtual reality (VR), generating 3D contents for these devices becomes an important topic in computer vision. For capturing full 360 degree panoramas in a single shot, the Spherical Panoramic Camera (SPC) are gaining in popularity. However, estimating depth from a SPC remains a challenging problem. In this paper, we propose a practical method that generates all-around dense depth map using a narrow-baseline video clip captured by a SPC. While existing methods for depth from small motion rely on perspective cameras, we introduce a new bundle adjustment approach tailored for SPC that minimizes the re-projection error directly on the unit sphere. It enables to estimate approximate metric camera poses and 3D points. Additionally, we present a novel dense matching method called sphere sweeping algorithm. This allows us to take advantage of the overlapping regions between the cameras. To validate the effectiveness of the proposed method, we evaluate our approach on both synthetic and real-world data. As an example of the applications, we also present stereoscopic panorama images generated from our depth results.},
  doi       = {10.1007/978-3-319-46487-9_10},
}

@InProceedings{JiangHKPMN2019,
  author    = {Chiyu "Max" Jiang and Jingwei Huang and Karthik Kashinath and Prabhat and Philip Marcus and Matthias Niessner},
  title     = {Spherical {CNNs} on Unstructured Grids},
  booktitle = ICLR,
  year      = {2019},
  abstract  = {We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters. Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation. As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters. We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation. Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.},
  arxiv     = {1901.02039},
  _url       = {https://arxiv.org/abs/1901.02039},
}

@Article{JiangSZDH2021,
  author    = {Hualie Jiang and Zhe Sheng and Siyu Zhu and Zilong Dong and Rui Huang},
  title     = {{UniFuse}: Unidirectional Fusion for 360° Panorama Depth Estimation},
  journal   = {IEEE Robotics and Automation Letters},
  year      = {2021},
  volume    = {6},
  number    = {2},
  pages     = {1519--1526},
  _month     = apr,
  _issn      = {2377-3766},
  abstract  = {Learning depth from spherical panoramas is becoming a popular research topic because a panorama has a full field-of-view of the environment and provides a relatively complete description of a scene. However, applying well-studied CNNs for perspective images to the standard representation of spherical panoramas, i.e., the equirectangular projection, is suboptimal, as it becomes distorted towards the poles. Another representation is the cubemap projection, which is distortion-free but discontinued on edges and limited in the field-of-view. This letter introduces a new framework to fuse features from the two projections, unidirectionally feeding the cubemap features to the equirectangular features only at the decoding stage. Unlike the recent bidirectional fusion approach operating at both the encoding and decoding stages, our fusion scheme is much more efficient. Besides, we also designed a more effective fusion module for our fusion scheme. Experiments verify the effectiveness of our proposed fusion strategy and module, and our model achieves state-of-the-art performance on four popular datasets. Additional experiments show that our model also has the advantages of model complexity and generalization capability.},
  doi       = {10.1109/LRA.2021.3058957},
}

@InProceedings{JinXZZTXYG2020,
  author    = {Lei Jin and Yanyu Xu and Jia Zheng and Junfei Zhang and Rui Tang and Shugong Xu and Jingyi Yu and Shenghua Gao},
  title     = {Geometric Structure Based and Regularized Depth Estimation From 360 Indoor Imagery},
  booktitle = CVPR,
  year      = {2020},
  pages     = {886--895},
  _month     = jun,
  abstract  = {Motivated by the correlation between the depth and the geometric structure of a 360 indoor image, we propose a novel learning-based depth estimation framework that leverages the geometric structure of a scene to conduct depth estimation. Specifically, we represent the geometric structure of an indoor scene as a collection of corners, boundaries and planes. On the one hand, once a depth map is estimated, this geometric structure can be inferred from the estimated depth map; thus, the geometric structure functions as a regularizer for depth estimation. On the other hand, this estimation also benefits from the geometric structure of a scene estimated from an image where the structure functions as a prior. However, furniture in indoor scenes makes it challenging to infer geometric structure from depth or image data. An attention map is inferred to facilitate both depth estimation from features of the geometric structure and also geometric inferences from the estimated depth map. To validate the effectiveness of each component in our framework under controlled conditions, we render a synthetic dataset, Shanghaitech-Kujiale Indoor 360 dataset with 3550 360 indoor images. Extensive experiments on popular datasets validate the effectiveness of our solution. We also demonstrate that our method can also be applied to counterfactual depth.},
  doi       = {10.1109/CVPR42600.2020.00097},
  _url       = {https://svip-lab.github.io/dataset/indoor_360.html},
}

@Article{Kopf2016,
  author    = {Johannes Kopf},
  title     = {360° Video Stabilization},
  journal   = ProcSIGASIA,
  year      = {2016},
  volume    = {35},
  number    = {6},
  pages     = {195:1--9},
  _month     = nov,
  _issn      = {0730-0301},
  abstract  = {We present a hybrid 3D-2D algorithm for stabilizing 360° video using a deformable rotation motion model. Our algorithm uses 3D analysis to estimate the rotation between key frames that are appropriately spaced such that the right amount of motion has occurred to make that operation reliable. For the remaining frames, it uses 2D optimization to maximize the visual smoothness of feature point trajectories. A new low-dimensional flexible deformed rotation motion model enables handling small translational jitter, parallax, lens deformation, and rolling shutter wobble. Our 3D-2D architecture achieves better robustness, speed, and smoothing ability than either pure 2D or 3D methods can provide. Stabilizing a video with our method takes less time than playing it at normal speed. The results are sufficiently smooth to be played back at high speed-up factors; for this purpose we present a simple 360° hyperlapse algorithm that remaps the video frame time stamps to balance the apparent camera velocity.},
  doi       = {10.1145/2980179.2982405},
  keywords  = {360 video, video stabilization},
}

@InProceedings{KroegTDV2016,
  author    = {Till Kroeger and Radu Timofte and Dengxin Dai and Van Gool, Luc},
  title     = {Fast Optical Flow Using Dense Inverse Search},
  booktitle = ECCV,
  year      = {2016},
  _editor    = {Bastian Leibe and Jiri Matas and Nicu Sebe and Max Welling},
  pages     = {471--488},
  abstract  = {Most recent works in optical flow extraction focus on the accuracy and neglect the time complexity. However, in real-life visual applications, such as tracking, activity detection and recognition, the time complexity is critical. We propose a solution with very low time complexity and competitive accuracy for the computation of dense optical flow. It consists of three parts: (1) inverse search for patch correspondences; (2) dense displacement field creation through patch aggregation along multiple scales; (3) variational refinement. At the core of our Dense Inverse Search-based method (DIS) is the efficient search of correspondences inspired by the inverse compositional image alignment proposed by Baker and Matthews (2001, 2004). DIS is competitive on standard optical flow benchmarks. DIS runs at 300 Hz up to 600 Hz on a single CPU core (1024×436 resolution. 42 Hz/46 Hz when including preprocessing: disk access, image re-scaling, gradient computation. More details in Sect. 3.1.), reaching the temporal resolution of human’s biological vision system. It is order(s) of magnitude faster than state-of-the-art methods in the same range of accuracy, making DIS ideal for real-time applications.},
  arxiv     = {1603.03590},
  doi       = {10.1007/978-3-319-46493-0_29},
  _isbn      = {978-3-319-46493-0},
}

@InProceedings{LaiXLL2019,
  author    = {Po Kong Lai and Shuang Xie and Jochen Lang and Robert Laganière},
  title     = {Real-time panoramic depth maps from omni-directional stereo images for 6 {DoF} videos in virtual reality},
  booktitle = VR,
  year      = {2019},
  pages     = {405--412},
  _month     = mar,
  abstract  = {In this paper we present an approach for 6 DoF panoramic videos from omni-directional stereo (ODS) images using convolutional neural networks (CNNs). More specifically, we use CNNs to generate panoramic depth maps from ODS images in real-time. These depth maps would then allow for re-projection of panoramic images thus providing 6 DoF to a viewer in virtual reality (VR). As the boundaries of a panoramic image must touch in order to envelope a viewer, we introduce a border weighted loss function as well as new error metrics specifically tailored for panoramic images. We show experimentally that training with our border weighted loss function improves performance by benchmarking a baseline skip-connected encoder-decoder style network as well as other state-of-the-art methods in depth map estimation from mono and stereo images. Finally, a practical application for VR using real world data is also demonstrated.},
  doi       = {10.1109/VR.2019.8798016},
  _issn      = {2642-5254},
  _url       = {https://github.com/pokonglai/ods-net},
}

@InProceedings{LeeJYJY2019,
  author    = {Yeon Kun Lee and Jaeseok Jeong and Jong Seob Yun and Cho Won June and Kuk-Jin Yoon},
  title     = {{SpherePHD}: Applying {CNNs} on a Spherical {PolyHeDron} Representation of 360° Images},
  booktitle = CVPR,
  year      = {2019},
  pages     = {9173--9181},
  _month     = jun,
  abstract  = {Omni-directional cameras have many advantages over conventional cameras in that they have a much wider field-of-view (FOV). Several approaches have been recently proposed to apply convolutional neural networks (CNNs) to omni-directional images to solve classification and detection problems. However, most of them use image representations in the Euclidean space defined by transforming the omni-directional views originally in the non-Euclidean space. This transformation leads to shape distortion due to nonuniform spatial resolving power and loss of continuity. These effects make existing convolution kernels have difficulties in extracting meaningful information. This paper proposes a novel method to resolve the aforementioned problems of applying CNNs to omni-directional images. The proposed method utilizes a spherical polyhedron to represent omni-directional views. This method minimizes the variance of spatial resolving power on the sphere surface, and includes new convolution and pooling methods for the proposed representation. The proposed approach can also be adopted by existing CNN-based methods. The feasibility and efficacy of the proposed method is demonstrated through both classification and detection tasks.},
  arxiv     = {1811.08196},
  doi       = {10.1109/CVPR.2019.00940},
  _issn      = {1063-6919},
  _url       = {https://arxiv.org/abs/1811.08196},
}

@Article{Li2008,
  author    = {Shigang Li},
  title     = {Binocular Spherical Stereo},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  year      = {2008},
  volume    = {9},
  number    = {4},
  pages     = {589--600},
  _month     = dec,
  _issn      = {1524-9050},
  abstract  = {A fish-eye camera has a wide field of view (FOV), and the realization of a binocular fish-eye stereo for sensing the surrounding 3-D information of the environment around a vehicle is useful for safe driving. However, since a fish-eye camera may have a wider-than-hemispherical FOV, the conventional stereo approach of obtaining a perspective image based on the pinhole camera model cannot directly be applied. However, using a spherical camera model and defining the disparity of a spherical stereo, the conventional binocular stereo problem is reformulated as a binocular spherical stereo problem. A binocular spherical stereo is a generalized paradigm that can cope with cameras having any FOV, including conventional cameras and fish-eye cameras. Moreover, by transforming the rectified spherical images to latitude-longitude representation, the feature point matching of the spherical stereo images can be sped up by using the processing used for perspective stereo images. The effectiveness of this approach is demonstrated by realizing a binocular spherical stereo using a pair of fish-eye cameras. Finally, the application of the proposed approach to vehicles in the future is considered.},
  doi       = {10.1109/TITS.2008.2006736},
  keywords  = {3-D information, driving assistance, fish-eye camera, stereo vision},
}

@Article{LiDCTSLF2021,
  author    = {Zhengqi Li and Tali Dekel and Forrester Cole and Richard Tucker and Noah Snavely and Ce Liu and William T. Freeman},
  title     = {{MannequinChallenge}: Learning the Depths of Moving People by Watching Frozen People},
  journal   = PAMI,
  year      = {2021},
  _issn      = {1939-3539},
  abstract  = {We present a method for predicting dense depth in scenarios where both a monocular camera and people in the scene are freely moving. Existing methods for recovering depth for dynamic, non-rigid objects from monocular video impose strong assumptions on the objects' motion and may only recover sparse depth. In this paper, we take a data-driven approach and learn human depth priors from a new source of data: thousands of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a hand-held camera tours the scene. Because people are stationary, training data can be generated using multi-view stereo reconstruction. At inference time, our method uses motion parallax cues from the static areas of the scenes to guide the depth prediction. We demonstrate our method on real-world sequences of complex human actions captured by a moving hand-held camera, show improvement over state-of-the-art monocular depth prediction methods, and show various 3D effects produced using our predicted depth.},
  doi       = {10.1109/TPAMI.2020.2974454},
  _url       = {https://mannequin-depth.github.io/},
}

@InProceedings{LucasK1981,
  author    = {Bruce D. Lucas and Takeo Kanade},
  title     = {An iterative image registration technique with an application to stereo vision},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
  year      = {1981},
  abstract  = {Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is faster because it examines far fewer potential matches between the images than existing techniques. Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show show our technique can be adapted for use in a stereo vision system.},
}

@Article{LuoZSX2019,
  author    = {Junren Luo and Wanpeng Zhang and Jiongming Su and Fengtao Xiang},
  title     = {Hexagonal Convolutional Neural Networks for Hexagonal Grids},
  journal   = {IEEE Access},
  year      = {2019},
  volume    = {7},
  pages     = {142738--142749},
  _issn      = {2169-3536},
  abstract  = {Hexagonal grids use a hierarchical subdivision tessellation to cover the entire plane or sphere. Due to the 6-fold rotational symmetry, hexagonal grids have some advantages (e.g. isoperimetry, equidistant neighbors, and uniform connectivity) over quadrangular and triangular girds, which makes them suitable to tackle tasks of geospatial information processing and intelligent decision-making. In this paper, we first introduce some applications based on the hexagonal grids. Then, we introduce the planer and spherical hexagonal grids and analyze the group representations for them, we review geometric deep learning, some Convolutional Neural Networks (CNNs) for hexagonal grids, and group-based equivariant convolution. Next in importance, we propose the HexagonNet for hexagonal grids, and define a new convolution operator and pooling operator. Finally, in order to evaluate the effectiveness of the proposed HexagonNet, we perform experiments on two tasks: aerial scene classification on the aerial image dataset (AID), and 3D shape classification on the ModelNet40 dataset. The experimental results verify the practical applicability of the HexagonNet given some fixed parameter budgets.},
  doi       = {10.1109/ACCESS.2019.2944766},
}

@Article{MatzeCEKS2017,
  author    = {Kevin Matzen and Michael F. Cohen and Bryce Evans and Johannes Kopf and Richard Szeliski},
  title     = {Low-cost 360 Stereo Photography and Video Capture},
  journal   = ProcSIGGRAPH,
  year      = {2017},
  volume    = {36},
  number    = {4},
  pages     = {148:1--12},
  _month     = jul,
  _issn      = {0730-0301},
  abstract  = {A number of consumer-grade spherical cameras have recently appeared, enabling affordable monoscopic VR content creation in the form of full 360° X 180° spherical panoramic photos and videos. While monoscopic content is certainly engaging, it fails to leverage a main aspect of VR HMDs, namely stereoscopic display. Recent stereoscopic capture rigs involve placing many cameras in a ring and synthesizing an omni-directional stereo panorama enabling a user to look around to explore the scene in stereo. In this work, we describe a method that takes images from two 360° spherical cameras and synthesizes an omni-directional stereo panorama with stereo in all directions. Our proposed method has a lower equipment cost than camera-ring alternatives, can be assembled with currently available off-the-shelf equipment, and is relatively small and light-weight compared to the alternatives. We validate our method by generating both stills and videos. We have conducted a user study to better understand what kinds of geometric processing are necessary for a pleasant viewing experience. We also discuss several algorithmic variations, each with their own time and quality trade-offs.},
  doi       = {10.1145/3072959.3073645},
  keywords  = {image stitching, panoramas, stereo, virtual reality},
}

@Article{MenzeHG2018,
  author    = {Moritz Menze and Christian Heipke and Andreas Geiger},
  title     = {Object Scene Flow},
  journal   = {ISPRS Journal of Photogrammetry and Remote Sensing},
  year      = {2018},
  volume    = {140},
  pages     = {60--76},
  _month     = jun,
  _issn      = {0924-2716},
  abstract  = {This work investigates the estimation of dense three-dimensional motion fields, commonly referred to as scene flow. While great progress has been made in recent years, large displacements and adverse imaging conditions as observed in natural outdoor environments are still very challenging for current approaches to reconstruction and motion estimation. In this paper, we propose a unified random field model which reasons jointly about 3D scene flow as well as the location, shape and motion of vehicles in the observed scene. We formulate the problem as the task of decomposing the scene into a small number of rigidly moving objects sharing the same motion parameters. Thus, our formulation effectively introduces long-range spatial dependencies which commonly employed local rigidity priors are lacking. Our inference algorithm then estimates the association of image segments and object hypotheses together with their three-dimensional shape and motion. We demonstrate the potential of the proposed approach by introducing a novel challenging scene flow benchmark which allows for a thorough comparison of the proposed scene flow approach with respect to various baseline models. In contrast to previous benchmarks, our evaluation is the first to provide stereo and optical flow ground truth for dynamic real-world urban scenes at large scale. Our experiments reveal that rigid motion segmentation can be utilized as an effective regularizer for the scene flow problem, improving upon existing two-frame scene flow methods. At the same time, our method yields plausible object segmentations without requiring an explicitly trained recognition model for a specific object class.},
  doi       = {10.1016/j.isprsjprs.2017.09.013},
  keywords  = {scene flow, motion estimation, motion segmentation, 3D reconstruction, active shape model, object detection},
}

@InProceedings{PintoAASG2021,
  author    = {Giovanni Pintore and Marco Agus and Eva Almansa and Jens Schneider and Enrico Gobbetti},
  title     = {{SliceNet}: Deep Dense Depth Estimation From a Single Indoor Panorama Using a Slice-Based Representation},
  booktitle = CVPR,
  year      = {2021},
  pages     = {11536--11545},
  abstract  = {We introduce a novel deep neural network to estimate a depth map from a single monocular indoor panorama. The network directly works on the equirectangular projection, exploiting the properties of indoor 360 images. Starting from the fact that gravity plays an important role in the design and construction of man-made indoor scenes, we propose a compact representation of the scene into vertical slices of the sphere, and we exploit long- and short-term relationships among slices to recover the equirectangular depth map. Our design makes it possible to maintain high-resolution information in the extracted features even with a deep network. The experimental results demonstrate that our method outperforms current state-of-the-art solutions in prediction accuracy, particularly for real-world data.},
  _url       = {https://openaccess.thecvf.com/content/CVPR2021/html/Pintore_SliceNet_Deep_Dense_Depth_Estimation_From_a_Single_Indoor_Panorama_CVPR_2021_paper.html},
}

@Article{RanftLHSK2021,
  author    = {René Ranftl and Katrin Lasinger and David Hafner and Konrad Schindler and Vladlen Koltun},
  title     = {Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer},
  journal   = PAMI,
  year      = {2021},
  _issn      = {1939-3539},
  abstract  = {The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation. Some results are shown in the supplementary video at https://youtu.be/D46FzVyL9I8},
  arxiv     = {1907.01341},
  doi       = {10.1109/TPAMI.2020.3019967},
  _url       = {https://github.com/intel-isl/MiDaS},
}

@InProceedings{SaikiMZHB2019,
  author    = {Tonmoy Saikia and Yassine Marrakchi and Arber Zela and Frank Hutter and Thomas Brox},
  title     = {{AutoDispNet}: Improving Disparity Estimation With {AutoML}},
  booktitle = ICCV,
  year      = {2019},
  pages     = {1812--1823},
  _month     = oct,
  abstract  = {Much research work in computer vision is being spent on optimizing existing network architectures to obtain a few more percentage points on benchmarks. Recent AutoML approaches promise to relieve us from this effort. However, they are mainly designed for comparatively small-scale classification tasks. In this work, we show how to use and extend existing AutoML techniques to efficiently optimize large-scale U-Net-like encoder-decoder architectures. In particular, we leverage gradient-based neural architecture search and Bayesian optimization for hyperparameter search. The resulting optimization does not require a large-scale compute cluster. We show results on disparity estimation that clearly outperform the manually optimized baseline and reach state-of-the-art performance.},
  doi       = {10.1109/ICCV.2019.00190},
  _url       = {https://github.com/lmb-freiburg/autodispnet},
}

@InProceedings{SavvaKMZWJSLKMPB2019,
  author    = {Manolis Savva and Abhishek Kadian and Oleksandr Maksymets and Yili Zhao and Erik Wijmans and Bhavana Jain and Julian Straub and Jia Liu and Vladlen Koltun and Jitendra Malik and Devi Parikh and Dhruv Batra},
  title     = {{Habitat}: A Platform for Embodied {AI} Research},
  booktitle = ICCV,
  year      = {2019},
  pages     = {9339--9347},
  _month     = oct,
  abstract  = {We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments  train, test  x  Matterport3D, Gibson  for multiple sensors  blind, RGB, RGBD, D  and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.},
  doi       = {10.1109/ICCV.2019.00943},
  _issn      = {1550-5499},
  _url       = {http://openaccess.thecvf.com/content_ICCV_2019/html/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.html},
}

@Unpublished{StrauWMCWGEMRVCYBYPYZLCBGMPSBSNGLN2019,
  author    = {Julian Straub and Thomas Whelan and Lingni Ma and Yufan Chen and Erik Wijmans and Simon Green and Jakob J. Engel and Raul Mur-Artal and Carl Ren and Shobhit Verma and Anton Clarkson and Mingfei Yan and Brian Budge and Yajie Yan and Xiaqing Pan and June Yon and Yuyang Zou and Kimberly Leon and Nigel Carter and Jesus Briales and Tyler Gillingham and Elias Mueggler and Luis Pesqueira and Manolis Savva and Dhruv Batra and Hauke M. Strasdat and Renzo De Nardi and Michael Goesele and Steven Lovegrove and Richard Newcombe},
  title     = {The {Replica} Dataset: A Digital Replica of Indoor Spaces},
  note      = {arXiv:\href{https://arxiv.org/abs/1906.05797}{1906.05797}},
  year      = {2019},
  abstract  = {We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world - for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is `Habitat-compatible', i.e. can be natively used with AI Habitat for training and testing embodied agents.},
  arxiv     = {1906.05797},
  _url       = {https://github.com/facebookresearch/Replica-Dataset},
}

@InProceedings{SuG2019,
  author    = {Yu-Chuan Su and Kristen Grauman},
  title     = {Kernel Transformer Networks for Compact Spherical Convolution},
  booktitle = CVPR,
  year      = {2019},
  pages     = {9442--9451},
  _month     = jun,
  abstract  = {Ideally, 360° imagery could inherit the deep convolutional neural networks (CNNs) already trained with great success on perspective projection images. However, existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. In this work, we present the Kernel Transformer Network (KTN). KTNs efficiently transfer convolution kernels from perspective images to the equirectangular projection of 360° images. Given a source CNN for perspective images as input, the KTN produces a function parameterized by a polar angle and kernel as output. Given a novel 360° image, that function in turn can compute convolutions for arbitrary layers and kernels as would the source CNN on the corresponding tangent plane projections. Distinct from all existing methods, KTNs allow model transfer: the same model can be applied to different source CNNs with the same base architecture. This enables application to multiple recognition tasks without re-training the KTN. Validating our approach with multiple source CNNs and datasets, we show that KTNs improve the state of the art for spherical convolution. KTNs successfully preserve the source CNN's accuracy, while offering transferability, scalability to typical image resolutions, and, in many cases, a substantially lower memory footprint.},
  arxiv     = {1812.03115},
  doi       = {10.1109/CVPR.2019.00967},
  _issn      = {1063-6919},
  _url       = {https://github.com/sammy-su/KernelTransformerNetwork},
}

@InProceedings{SuG2017a,
  author    = {Yu-Chuan Su and Kristen Grauman},
  title     = {Learning Spherical Convolution for Fast Features from 360° Imagery},
  booktitle = NIPS,
  year      = {2017},
  abstract  = {While 360° cameras offer tremendous new possibilities in vision, graphics, and augmented reality, the spherical images they produce make core feature extraction non-trivial. Convolutional neural networks (CNNs) trained on images from perspective cameras yield "flat" filters, yet 360° images cannot be projected to a single plane without significant distortion. A naive solution that repeatedly projects the viewing sphere to all tangent planes is accurate, but much too computationally intensive for real problems. We propose to learn a spherical convolutional network that translates a planar CNN to process 360° imagery directly in its equirectangular projection. Our approach learns to reproduce the flat filter outputs on 360° data, sensitive to the varying distortion effects across the viewing sphere. The key benefits are 1) efficient feature extraction for 360° images and video, and 2) the ability to leverage powerful pre-trained networks researchers have carefully honed (together with massive labeled image training sets) for perspective images. We validate our approach compared to several alternative methods in terms of both raw CNN output accuracy as well as applying a state-of-the-art "flat" object detector to 360° data. Our method yields the most accurate results while saving orders of magnitude in computation versus the existing exact reprojection solution.},
  arxiv     = {1708.00919},
  _url       = {https://arxiv.org/abs/1708.00919},
}

@InProceedings{SunSC2021,
  author        = {Cheng Sun and Min Sun and Hwann-Tzong Chen},
  title         = {{HoHoNet}: 360 Indoor Holistic Understanding with Latent Horizontal Features},
  booktitle     = CVPR,
  year          = {2021},
  abstract      = {We present HoHoNet, a versatile and efficient framework for holistic understanding of an indoor 360-degree panorama using a Latent Horizontal Feature (LHFeat). The compact LHFeat flattens the features along the vertical direction and has shown success in modeling per-column modality for room layout reconstruction. HoHoNet advances in two important aspects. First, the deep architecture is redesigned to run faster with improved accuracy. Second, we propose a novel horizon-to-dense module, which relaxes the per-column output shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones respectively, for modeling dense modalities from a high-resolution $512 \times 1024$ panorama. HoHoNet is also accurate. On the tasks of layout estimation and semantic segmentation, HoHoNet achieves results on par with current state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior arts by a large margin.},
  arxiv         = {2011.11498},
  _url           = {https://github.com/sunset1995/HoHoNet},
}

@InProceedings{SunRB2010,
  author    = {Deqing Sun and S. Roth and M.J. Black},
  title     = {Secrets of optical flow estimation and their principles},
  booktitle = CVPR,
  year      = {2010},
  pages     = {2432--2439},
  _month     = jun,
  abstract  = {The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that #x201C;classical #x201D; flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. Moreover, we find that while median filtering of intermediate flow fields during optimization is a key to recent performance gains, it leads to higher energy solutions. To understand the principles behind this phenomenon, we derive a new objective that formalizes the median filtering heuristic. This objective includes a nonlocal term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that ranks at the top of the Middlebury benchmark.},
  doi       = {10.1109/CVPR.2010.5539939},
  _issn      = {1063-6919},
}

@Article{SunRB2014,
  author    = {Deqing Sun and Stefan Roth and Michael J. Black},
  title     = {A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them},
  journal   = IJCV,
  year      = {2014},
  volume    = {106},
  number    = {2},
  pages     = {115--137},
  _month     = jan,
  _issn      = {0920-5691},
  abstract  = {The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that “classical” flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. One key implementation detail is the median filtering of intermediate flow fields during optimization. While this improves the robustness of classical methods it actually leads to higher energy solutions, meaning that these methods are not optimizing the original objective function. To understand the principles behind this phenomenon, we derive a new objective function that formalizes the median filtering heuristic. This objective function includes a non-local smoothness term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that can better preserve motion details. To take advantage of the trend towards video in wide-screen format, we further introduce an asymmetric pyramid downsampling scheme that enables the estimation of longer range horizontal motions. The methods are evaluated on the Middlebury, MPI Sintel, and KITTI datasets using the same parameter settings.},
  doi       = {10.1007/s11263-013-0644-x},
  keywords  = {optical flow estimation, practices, median filtering, non-local term, motion boundary},
}

@InProceedings{SunSB2010,
  author    = {Deqing Sun and Erik B. Sudderth and Michael J. Black},
  title     = {Layered image motion with explicit occlusions, temporal consistency, and depth ordering},
  booktitle = NIPS,
  year      = {2010},
  pages     = {2226--2234},
  abstract  = {Layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other. For image motion estimation, such models have a long history but have not achieved the wide use or accuracy of non-layered methods. We present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches. In particular, we define a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation. Additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an MRF with a robust spatial prior; the resulting model allows roughness in layers. Finally, a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent models for static scene segmentation. The method achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions.},
  _url       = {http://papers.nips.cc/paper/4030-layered-image-motion-with-explicit-occlusions-temporal-consistency-and-depth-ordering},
}

@InProceedings{SunSP2015,
  author    = {Deqing Sun and Erik B. Sudderth and Hanspeter Pfister},
  title     = {Layered {RGBD} Scene Flow Estimation},
  booktitle = CVPR,
  year      = {2015},
  pages     = {548--556},
  _month     = jun,
  abstract  = {As consumer depth sensors become widely available, estimating scene flow from RGBD sequences has received increasing attention. Although the depth information allows the recovery of 3D motion from a single view, it poses new challenges. In particular, depth boundaries are not well-aligned with RGB image edges and therefore not reliable cues to localize 2D motion boundaries. In addition, methods that extend the 2D optical flow formulation to 3D still produce large errors in occlusion regions. To better use depth for occlusion reasoning, we propose a layered RGBD scene flow method that jointly solves for the scene segmentation and the motion. Our key observation is that the noisy depth is sufficient to decide the depth ordering of layers, thereby avoiding a computational bottleneck for RGB layered methods. Furthermore, the depth enables us to estimate a per-layer 3D rigid motion to constrain the motion of each layer. Experimental results on both the Middlebury and real-world sequences demonstrate the effectiveness of the layered approach for RGBD scene flow estimation.},
  doi       = {10.1109/CVPR.2015.7298653},
}

@Article{SunYLK2020,
  author    = {Deqing Sun and Xiaodong Yang and Ming-Yu Liu and Jan Kautz},
  title     = {Models Matter, So Does Training: An Empirical Study of {CNNs} for Optical Flow Estimation},
  journal   = PAMI,
  year      = {2020},
  volume    = {42},
  number    = {6},
  pages     = {1408--1423},
  _month     = jun,
  _issn      = {1939-3539},
  abstract  = {We investigate two crucial and closely-related aspects of CNNs for optical flow estimation: models and training. First, we design a compact but effective CNN model, called PWC-Net, according to simple and well-established principles: pyramidal processing, warping, and cost volume processing. PWC-Net is 17 times smaller in size, 2 times faster in inference, and 11 percent more accurate on Sintel final than the recent FlowNet2 model. It is the winning entry in the optical flow competition of the robust vision challenge. Next, we experimentally analyze the sources of our performance gains. In particular, we use the same training procedure for PWC-Net to retrain FlowNetC, a sub-network of FlowNet2. The retrained FlowNetC is 56 percent more accurate on Sintel final than the previously trained one and even 5 percent more accurate than the FlowNet2 model. We further improve the training procedure and increase the accuracy of PWC-Net on Sintel by 10 percent and on KITTI 2012 and 2015 by 20 percent. Our newly trained model parameters and training protocols are available on https://github.com/NVlabs/PWC-Net.},
  arxiv     = {1809.05571},
  doi       = {10.1109/TPAMI.2019.2894353},
  _url       = {https://arxiv.org/abs/1809.05571},
}

@InProceedings{SunYLK2018,
  author    = {Deqing Sun and Xiaodong Yang and Ming-Yu Liu and Jan Kautz},
  title     = {{PWC-Net}: {CNNs} for Optical Flow Using Pyramid, Warping, and Cost Volume},
  booktitle = CVPR,
  year      = {2018},
  pages     = {8934--8943},
  _month     = jun,
  abstract  = {We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the current optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our models are available on our project website.},
  arxiv     = {1709.02371},
  doi       = {10.1109/CVPR.2018.00931},
  _url       = {http://openaccess.thecvf.com/content_cvpr_2018/html/Sun_PWC-Net_CNNs_for_CVPR_2018_paper.html},
}

@InProceedings{TatenNT2018,
  author    = {Keisuke Tateno and Nassir Navab and Federico Tombari},
  title     = {Distortion-Aware Convolutional Filters for Dense Prediction in Panoramic Images},
  booktitle = ECCV,
  year      = {2018},
  _editor    = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  pages     = {732--750},
  abstract  = {There is a high demand of 3D data for 360° panoramic images and videos, pushed by the growing availability on the market of specialized hardware for both capturing (e.g., omnidirectional cameras) as well as visualizing in 3D (e.g., head mounted displays) panoramic images and videos. At the same time, 3D sensors able to capture 3D panoramic data are expensive and/or hardly available. To fill this gap, we propose a learning approach for panoramic depth map estimation from a single image. Thanks to a specifically developed distortion-aware deformable convolution filter, our method can be trained by means of conventional perspective images, then used to regress depth for panoramic images, thus bypassing the effort needed to create annotated panoramic training dataset. We also demonstrate our approach for emerging tasks such as panoramic monocular SLAM, panoramic semantic segmentation and panoramic style transfer.},
  doi       = {10.1007/978-3-030-01270-0_43},
  _isbn      = {978-3-030-01270-0},
  _url       = {http://openaccess.thecvf.com/content_ECCV_2018/html/Keisuke_Tateno_Distortion-Aware_Convolutional_Filters_ECCV_2018_paper.html},
}

@InProceedings{TeedD2020a,
  author    = {Zachary Teed and Jia Deng},
  title     = {{RAFT}: Recurrent All-Pairs Field Transforms for Optical Flow},
  booktitle = ECCV,
  year      = {2020},
  abstract  = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10\%, a 16\% error reduction from the best published result (6.10\%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30\% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT},
  arxiv     = {2003.12039},
  doi       = {10.1007/978-3-030-58536-5_24},
  _url       = {https://github.com/princeton-vl/RAFT},
}

@InProceedings{Tran2021,
  author    = {Phi Vu Tran},
  title     = {{SSLayout360}: Semi-Supervised Indoor Layout Estimation from 360-Degree Panorama},
  booktitle = CVPR,
  year      = {2021},
  abstract  = {Recent years have seen flourishing research on both semi-supervised learning and 3D room layout reconstruction. In this work, we explore the intersection of these two fields to advance the research objective of enabling more accurate 3D indoor scene modeling with less labeled data. We propose the first approach to learn representations of room corners and boundaries by using a combination of labeled and unlabeled data for improved layout estimation in a 360-degree panoramic scene. Through extensive comparative experiments, we demonstrate that our approach can advance layout estimation of complex indoor scenes using as few as 20 labeled examples. When coupled with a layout predictor pre-trained on synthetic data, our semi-supervised method matches the fully supervised counterpart using only 12% of the labels. Our work takes an important first step towards robust semi-supervised layout estimation that can enable many applications in 3D perception with limited labeled data.},
  arxiv     = {2103.13696},
  _url       = {https://github.com/FlyreelAI/sslayout360},
}

@Article{Tsai1987,
  author    = {Roger Y. Tsai},
  title     = {A versatile camera calibration technique for high-accuracy {3D} machine vision metrology using off-the-shelf {TV} cameras and lenses},
  journal   = {IEEE Journal on Robotics and Automation},
  year      = {1987},
  volume    = {3},
  number    = {4},
  pages     = {323--344},
  _month     = aug,
  _issn      = {2374-8710},
  abstract  = {A new technique for three-dimensional (3D) camera calibration for machine vision metrology using off-the-shelf TV cameras and lenses is described. The two-stage technique is aimed at efficient computation of camera external position and orientation relative to object reference coordinate system as well as the effective focal length, radial lens distortion, and image scanning parameters. The two-stage technique has advantage in terms of accuracy, speed, and versatility over existing state of the art. A critical review of the state of the art is given in the beginning. A theoretical framework is established, supported by comprehensive proof in five appendixes, and may pave the way for future research on 3D robotics vision. Test results using real data are described. Both accuracy and speed are reported. The experimental results are analyzed and compared with theoretical prediction. Recent effort indicates that with slight modification, the two-stage calibration can be done in real time.},
  doi       = {10.1109/JRA.1987.1087109},
}

@InProceedings{WangHCLYSCS2018,
  author    = {Fu-En Wang and Hou-Ning Hu and Hsien-Tzu Cheng and Juan-Ting Lin and Shang-Ta Yang and Meng-Li Shih and Hung-Kuo Chu and Min Sun},
  title     = {Self-Supervised Learning of Depth and Camera Motion from 360° Videos},
  booktitle = ACCV,
  year      = {2018},
  pages     = {53--68},
  abstract  = {As 360° cameras become prevalent in many autonomous systems (e.g., self-driving cars and drones), efficient 360° perception becomes more and more important. We propose a novel self-supervised learning approach for predicting the omnidirectional depth and camera motion from a 360° video. In particular, starting from the SfMLearner, which is designed for cameras with normal field-of-view, we introduce three key features to process 360° images efficiently. Firstly, we convert each image from equirectangular projection to cubic projection in order to avoid image distortion. In each network layer, we use Cube Padding (CP), which pads intermediate features from adjacent faces, to avoid image boundaries. Secondly, we propose a novel "spherical" photometric consistency constraint on the whole viewing sphere. In this way, no pixel will be projected outside the image boundary which typically happens in images with normal field-of-view. Finally, rather than naively estimating six independent camera motions (i.e., naively applying SfM-Learner to each face on a cube), we propose a novel camera pose consistency loss to ensure the estimated camera motions reaching consensus. To train and evaluate our approach, we collect a new PanoSUNCG dataset containing a large amount of 360° videos with groundtruth depth and camera motion. Our approach achieves state-of-the-art depth prediction and camera motion estimation on PanoSUNCG with faster inference speed comparing to equirectangular. In real-world indoor videos, our approach can also achieve qualitatively reasonable depth prediction by acquiring model pre-trained on PanoSUNCG.},
  arxiv     = {1811.05304},
  doi       = {10.1007/978-3-030-20873-8_4},
  _isbn      = {978-3-030-20873-8},
  _url       = {http://aliensunmin.github.io/project/360-depth/},
}

@InProceedings{WangYSCT2021,
  author    = {Fu-En Wang and Yu-Hsuan Yeh and Min Sun and Wei-Chen Chiu and Yi-Hsuan Tsai},
  title     = {{LED$^2$}-Net: Monocular 360 Layout Estimation via Differentiable Depth Rendering},
  booktitle = CVPR,
  year      = {2021},
  abstract  = {Although significant progress has been made in room layout estimation, most methods aim to reduce the loss in the 2D pixel coordinate rather than exploiting the room structure in the 3D space. Towards reconstructing the room layout in 3D, we formulate the task of 360 layout estimation as a problem of predicting depth on the horizon line of a panorama. Specifically, we propose the Differentiable Depth Rendering procedure to make the conversion from layout to depth prediction differentiable, thus making our proposed model end-to-end trainable while leveraging the 3D geometric information, without the need of providing the ground truth depth. Our method achieves state-of-the-art performance on numerous 360 layout benchmark datasets. Moreover, our formulation enables a pre-training step on the depth dataset, which further improves the generalizability of our layout estimation model.},
  arxiv     = {2104.00568},
  _url       = {https://fuenwang.ml/project/led2net/},
}

@InProceedings{WangYSCT2020,
  author    = {Fu-En Wang and Yu-Hsuan Yeh and Min Sun and Wei-Chen Chiu and Yi-Hsuan Tsai},
  title     = {{BiFuse}: Monocular 360 Depth Estimation via Bi-Projection Fusion},
  booktitle = CVPR,
  year      = {2020},
  pages     = {462--471},
  _month     = jun,
  abstract  = {Depth estimation from a monocular 360 image is an emerging problem that gains popularity due to the availability of consumer-level 360 cameras and the complete surrounding sensing capability. While the standard of 360 imaging is under rapid development, we propose to predict the depth map of a monocular 360 image by mimicking both peripheral and foveal vision of the human eye. To this end, we adopt a two-branch neural network leveraging two common projections: equirectangular and cubemap projections. In particular, equirectangular projection incorporates a complete field-of-view but introduces distortion, whereas cubemap projection avoids distortion but introduces discontinuity at the boundary of the cube. Thus we propose a bi-projection fusion scheme along with learnable masks to balance the feature map from the two projections. Moreover, for the cubemap projection, we propose a spherical padding procedure which mitigates discontinuity at the boundary of each face. We apply our method to four panorama datasets and show favorable results against the existing state-of-the-art methods.},
  doi       = {10.1109/CVPR42600.2020.00054},
  _url       = {https://fuenwang.ml/project/bifuse/},
}

@InProceedings{WangSTCS2020,
  author    = {Ning-Hsu Wang and Bolivar Solarte and Yi-Hsuan Tsai and Wei-Chen Chiu and Min Sun},
  title     = {{360SD}-Net: 360° Stereo Depth Estimation with Learnable Cost Volume},
  booktitle = ICRA,
  year      = {2020},
  pages     = {582--588},
  abstract  = {Recently, end-to-end trainable deep neural networks have significantly improved stereo depth estimation for perspective images. However, 360° images captured under equirectangular projection cannot benefit from directly adopting existing methods due to distortion introduced (i.e., lines in 3D are not projected onto lines in 2D). To tackle this issue, we present a novel architecture specifically designed for spherical disparity using the setting of top-bottom 360° camera pairs. Moreover, we propose to mitigate the distortion issue by (1) an additional input branch capturing the position and relation of each pixel in the spherical coordinate, and (2) a cost volume built upon a learnable shifting filter. Due to the lack of 360° stereo data, we collect two 360° stereo datasets from Matterport3D and Stanford3D for training and evaluation. Extensive experiments and ablation study are provided to validate our method against existing algorithms. Finally, we show promising results on real-world environments capturing images with two consumer-level cameras. Our project page is at https://albert100121.github.io/360SD-Net-Project-Page.},
  arxiv     = {1911.04460},
  doi       = {10.1109/ICRA40945.2020.9196975},
  _issn      = {2577-087X},
  _url       = {https://albert100121.github.io/360SD-Net-Project-Page/},
}

@Article{WangBSS2004,
  author    = {Zhou Wang and Alan C. Bovik and Hamid R. Sheikh and Eero P. Simoncelli},
  title     = {Image quality assessment: from error visibility to structural similarity},
  journal   = TIP,
  year      = {2004},
  volume    = {13},
  number    = {4},
  pages     = {600--612},
  _month     = apr,
  abstract  = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  doi       = {10.1109/TIP.2003.819861},
  keywords  = {error sensitivity, human visual system (HVS), image coding, image quality assessment, JPEG, JPEG2000, perceptual quality, structural information, structural similarity (SSIM)},
}

@InProceedings{WonRL2019,
  author    = {Changhee Won and Jongbin Ryu and Jongwoo Lim},
  title     = {{OmniMVS}: End-to-End Learning for Omnidirectional Stereo Matching},
  booktitle = ICCV,
  year      = {2019},
  pages     = {8986--8995},
  abstract  = {In this paper, we propose a novel end-to-end deep neural network model for omnidirectional depth estimation from a wide-baseline multi-view stereo setup. The images captured with ultra wide field-of-view (FOV) cameras on an omnidirectional rig are processed by the feature extraction module, and then the deep feature maps are warped onto the concentric spheres swept through all candidate depths using the calibrated camera parameters. The 3D encoder-decoder block takes the aligned feature volume to produce the omnidirectional depth estimate with regularization on uncertain regions utilizing the global context information. In addition, we present large-scale synthetic datasets for training and testing omnidirectional multi-view stereo algorithms. Our datasets consist of 11K ground-truth depth maps and 45K fisheye images in four orthogonal directions with various objects and environments. Experimental results show that the proposed method generates excellent results in both synthetic and real-world environments, and it outperforms the prior art and the omnidirectional versions of the state-of-the-art conventional stereo algorithms.},
  arxiv     = {1908.06257},
  doi       = {10.1109/ICCV.2019.00908},
  _url       = {http://cvlab.hanyang.ac.kr/project/omnistereo/},
}

@InProceedings{WonRL2019a,
  author    = {Changhee Won and Jongbin Ryu and Jongwoo Lim},
  title     = {{SweepNet}: Wide-baseline Omnidirectional Depth Estimation},
  booktitle = ICRA,
  year      = {2019},
  abstract  = {Omnidirectional depth sensing has its advantage over the conventional stereo systems since it enables us to recognize the objects of interest in all directions without any blind regions. In this paper, we propose a novel wide-baseline omnidirectional stereo algorithm which computes the dense depth estimate from the fisheye images using a deep convolutional neural network. The capture system consists of multiple cameras mounted on a wide-baseline rig with ultrawide field of view (FOV) lenses, and we present the calibration algorithm for the extrinsic parameters based on the bundle adjustment. Instead of estimating depth maps from multiple sets of rectified images and stitching them, our approach directly generates one dense omnidirectional depth map with full 360-degree coverage at the rig global coordinate system. To this end, the proposed neural network is designed to output the cost volume from the warped images in the sphere sweeping method, and the final depth map is estimated by taking the minimum cost indices of the aggregated cost volume by SGM. For training the deep neural network and testing the entire system, realistic synthetic urban datasets are rendered using Blender. The experiments using the synthetic and real-world datasets show that our algorithm outperforms the conventional depth estimation methods and generate highly accurate depth maps.},
  arxiv     = {1902.10904},
  doi       = {10.1109/ICRA.2019.8793823},
  _url       = {https://arxiv.org/abs/1902.10904},
}

@InProceedings{WonSCPL2020,
  author    = {Changhee Won and Hochang Seok and Zhaopeng Cui and Marc Pollefeys and Jongwoo Lim},
  title     = {{OmniSLAM}: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems},
  booktitle = ICRA,
  year      = {2020},
  pages     = {559--566},
  abstract  = {In this paper, we present an omnidirectional localization and dense mapping system for a wide-baseline multiview stereo setup with ultra-wide field-of-view (FOV) fisheye cameras, which has a 360 degrees coverage of stereo observations of the environment. For more practical and accurate reconstruction, we first introduce improved and light-weighted deep neural networks for the omnidirectional depth estimation, which are faster and more accurate than the existing networks. Second, we integrate our omnidirectional depth estimates into the visual odometry (VO) and add a loop closing module for global consistency. Using the estimated depth map, we reproject keypoints onto each other view, which leads to a better and more efficient feature matching process. Finally, we fuse the omnidirectional depth maps and the estimated rig poses into the truncated signed distance function (TSDF) volume to acquire a 3D map. We evaluate our method on synthetic datasets with ground-truth and real-world sequences of challenging environments, and the extensive experiments show that the proposed system generates excellent reconstruction results in both synthetic and real-world environments.},
  arxiv     = {2003.08056},
  doi       = {10.1109/ICRA40945.2020.9196695},
}

@InProceedings{XuZXTG2021,
  author    = {Jiale Xu and Jia Zheng and Yanyu Xu and Rui Tang and Shenghua Gao},
  title     = {Layout-Guided Novel View Synthesis from a Single Indoor Panorama},
  booktitle = CVPR,
  year      = {2021},
  abstract  = {Existing view synthesis methods mainly focus on the perspective images and have shown promising results. However, due to the limited field-of-view of the pinhole camera, the performance quickly degrades when large camera movements are adopted. In this paper, we make the first attempt to generate novel views from a single indoor panorama and take the large camera translations into consideration. To tackle this challenging problem, we first use Convolutional Neural Networks (CNNs) to extract the deep features and estimate the depth map from the source-view image. Then, we leverage the room layout prior, a strong structural constraint of the indoor scene, to guide the generation of target views. More concretely, we estimate the room layout in the source view and transform it into the target viewpoint as guidance. Meanwhile, we also constrain the room layout of the generated target-view images to enforce geometric consistency. To validate the effectiveness of our method, we further build a large-scale photo-realistic dataset containing both small and large camera translations. The experimental results on our challenging dataset demonstrate that our method achieves state-of-the-art performance. The project page is at https://github.com/bluestyle97/PNVS

.},
  arxiv     = {2103.17022},
  _url       = {https://arxiv.org/abs/2103.17022},
}

@InProceedings{YangZRHS2021,
  author    = {Kailun Yang and Jiaming Zhang and Simon Reiß and Xinxin Hu and Rainer Stiefelhagen},
  title     = {Capturing Omni-Range Context for Omnidirectional Segmentation},
  booktitle = CVPR,
  year      = {2021},
  abstract  = {Convolutional Networks (ConvNets) excel at semantic segmentation and have become a vital component for perception in autonomous driving. Enabling an all-encompassing view of street-scenes, omnidirectional cameras present themselves as a perfect fit in such systems. Most segmentation models for parsing urban environments operate on common, narrow Field of View (FoV) images. Transferring these models from the domain they were designed for to 360-degree perception, their performance drops dramatically, e.g., by an absolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of FoV and structural distribution between the imaging domains, we introduce Efficient Concurrent Attention Networks (ECANets), directly capturing the inherent long-range dependencies in omnidirectional imagery. In addition to the learned attention-based contextual priors that can stretch across 360-degree images, we upgrade model training by leveraging multi-source and omni-supervised learning, taking advantage of both: Densely labeled and unlabeled data originating from multiple datasets. To foster progress in panoramic image segmentation, we put forward and extensively evaluate models on Wild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture diverse scenes from all around the globe. Our novel model, training regimen and multi-source prediction fusion elevate the performance (mIoU) to new state-of-the-art results on the public PASS (60.2%) and the fresh WildPASS (69.0%) benchmarks.},
  arxiv     = {2103.05687},
  _url       = {https://arxiv.org/abs/2103.05687},
}

@InProceedings{ZengKG2020,
  author    = {Wei Zeng and Sezer Karaoglu and Theo Gevers},
  title     = {Joint {3D} Layout and Depth Prediction from a Single Indoor Panorama Image},
  booktitle = ECCV,
  year      = {2020},
  abstract  = {In this paper, we propose a method which jointly learns the layout prediction and depth estimation from a single indoor panorama image. Previous methods have considered layout prediction and depth estimation from a single panorama image separately. However, these two tasks are tightly intertwined. Leveraging the layout depth map as an intermediate representation, our proposed method outperforms existing methods for both panorama layout prediction and depth estimation. Experiments on the challenging real-world dataset of Stanford 2D-3D demonstrate that our approach obtains superior performance for both the layout prediction tasks (3D IoU: 85.81% v.s. 79.79%) and the depth estimation (Abs Rel: 0.068 v.s. 0.079).},
  doi       = {10.1007/978-3-030-58517-4_39},
}

@InProceedings{ZhangLSC2019,
  author    = {Chao Zhang and Stephan Liwicki and William Smith and Roberto Cipolla},
  title     = {Orientation-Aware Semantic Segmentation on Icosahedron Spheres},
  booktitle = ICCV,
  year      = {2019},
  pages     = {3533--3541},
  _month     = oct,
  abstract  = {We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.},
  doi       = {10.1109/ICCV.2019.00363},
  _issn      = {1550-5499},
  _url       = {http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Orientation-Aware_Semantic_Segmentation_on_Icosahedron_Spheres_ICCV_2019_paper.html},
}

@InProceedings{ZhaoYZLBT2020,
  author    = {Pengyu Zhao and Ansheng You and Yuanxing Zhang and Jiaying Liu and Kaigui Bian and Yunhai Tong},
  title     = {Spherical Criteria for Fast and Accurate 360° Object Detection},
  booktitle = AAAI,
  year      = {2020},
  volume    = {34},
  pages     = {12959--12966},
  abstract  = {With the advance of omnidirectional panoramic technology, 360° imagery has become increasingly popular in the past few years. To better understand the 360° content, many works resort to the 360° object detection and various criteria have been proposed to bound the objects and compute the intersection-over-union (IoU) between bounding boxes based on the common equirectangular projection (ERP) or perspective projection (PSP). However, the existing 360° criteria are either inaccurate or inefficient for real-world scenarios. In this paper, we introduce a novel spherical criteria for fast and accurate 360° object detection, including both spherical bounding boxes and spherical IoU (SphIoU). Based on the spherical criteria, we propose a novel two-stage 360◦ detector, i.e., Reprojection R-CNN, by combining the advantages of both ERP and PSP, yielding efficient and accurate 360° object detection. To validate the design of spherical criteria and Reprojection R-CNN, we construct two unbiased synthetic datasets for training and evaluation. Experimental results reveal that compared with the existing criteria, the two-stage detector with spherical criteria achieves the best mAP results under the same inference speed, demonstrating that the spherical criteria can be more suitable for 360◦ object detection. Moreover, Reprojection R-CNN outperforms the previous state-of-the-art methods by over 30% on mAP with competitive speed, which confirms the efficiency and accuracy of the design.},
  doi       = {10.1609/aaai.v34i07.6995},
}

@Article{ZimmeBW2011a,
  author    = {Henning Zimmer and Andrés Bruhn and Joachim Weickert},
  title     = {Optic Flow in Harmony},
  journal   = IJCV,
  year      = {2011},
  volume    = {93},
  number    = {3},
  pages     = {368--388},
  _issn      = {0920-5691},
  abstract  = {Most variational optic flow approaches just consist of three constituents: a data term, a smoothness term and a smoothness weight. In this paper, we present an approach that harmonises these three components. We start by developing an advanced data term that is robust under outliers and varying illumination conditions. This is achieved by using constraint normalisation, and an HSV colour representation with higher order constancy assumptions and a separate robust penalisation. Our novel anisotropic smoothness is designed to work complementary to the data term. To this end, it incorporates directional information from the data constraints to enable a filling-in of information solely in the direction where the data term gives no information, yielding an optimal complementary smoothing behaviour. This strategy is applied in the spatial as well as in the spatio-temporal domain. Finally, we propose a simple method for automatically determining the optimal smoothness weight. This method bases on a novel concept that we call optimal prediction principle (OPP). It states that the flow field obtained with the optimal smoothness weight allows for the best prediction of the next frames in the image sequence. The benefits of our optic flow in harmony (OFH) approach are demonstrated by an extensive experimental validation and by a competitive performance at the widely used Middlebury optic flow benchmark.},
  doi       = {10.1007/s11263-011-0422-6},
  issue     = {3},
  keyword   = {Computer Science},
}

@InProceedings{ZioulKZAD2019,
  author    = {Nikolaos Zioulis and Antonis Karakottas and Dimitrios Zarpalas and Federico Alvarez and Petros Daras},
  title     = {Spherical View Synthesis for Self-Supervised 360° Depth Estimation},
  booktitle = Proc3DV,
  year      = {2019},
  pages     = {690--699},
  _month     = sep,
  abstract  = {Learning based approaches for depth perception are limited by the availability of clean training data. This has led to the utilization of view synthesis as an indirect objective for learning depth estimation using efficient data acquisition procedures. Nonetheless, most research focuses on pinhole based monocular vision, with scarce works presenting results for omnidirectional input. In this work, we explore spherical view synthesis for learning monocular 360 depth in a self-supervised manner and demonstrate its feasibility. Under a purely geometrically derived formulation we present results for horizontal and vertical baselines, as well as for the trinocular case. Further, we show how to better exploit the expressiveness of traditional CNNs when applied to the equirectangular domain in an efficient manner. Finally, given the availability of ground truth depth data, our work is uniquely positioned to compare view synthesis against direct supervision in a consistent and fair manner. The results indicate that alternative research directions might be better suited to enable higher quality depth perception. Our data, models and code are publicly available at https://vcl3d.github.io/SphericalViewSynthesis/
},
  arxiv     = {1909.08112},
  doi       = {10.1109/3DV.2019.00081},
  _issn      = {2378-3826},
  _url       = {https://arxiv.org/abs/1909.08112},
}

@InProceedings{ZioulKZD2018,
  author    = {Nikolaos Zioulis and Antonis Karakottas and Dimitrios Zarpalas and Petros Daras},
  title     = {{OmniDepth}: Dense Depth Estimation for Indoors Spherical Panoramas},
  booktitle = ECCV,
  year      = {2018},
  pages     = {448--465},
  _month     = sep,
  abstract  = {Recent work on depth estimation up to now has only focused on projective images ignoring 360 content which is now increasingly and more easily produced. We show that monocular depth estimation models trained on traditional images produce sub-optimal results on omnidirectional images, showcasing the need for training directly on 360 datasets, which however, are hard to acquire. In this work, we circumvent the challenges associated with acquiring high quality 360 datasets with ground truth depth annotations, by re-using recently released large scale 3D datasets and re-purposing them to 360 via rendering. This dataset, which is considerably larger than similar projective datasets, is publicly offered to the community to enable future research in this direction. We use this dataset to learn in an end-to-end fashion the task of depth estimation from 360 images. We show promising results in our synthesized data as well as in unseen realistic images.},
  doi       = {10.1007/978-3-030-01231-1_28},
  _url       = {http://openaccess.thecvf.com/content_ECCV_2018/html/NIKOLAOS_ZIOULIS_OmniDepth_Dense_Depth_ECCV_2018_paper.html},
}

@inproceedings{shugrina2019creative,
	title={Creative flow+ dataset},
	author={Shugrina, Maria and Liang, Ziheng and Kar, Amlan and Li, Jiaman and Singh, Angad and Singh, Karan and Fidler, Sanja},
	booktitle= CVPR,
	pages={5384--5393},
	year={2019}
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;author;false;editor;false;year;true;}
