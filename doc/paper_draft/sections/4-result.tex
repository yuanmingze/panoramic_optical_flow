\section{Experiments}\label{sec:exp}

To evaluate the performation
\TODO{Time} 
\TODO{How much time it will take for .}
\TODO{How to select the weight blending weight.}

\subsection{Dataset \& Error Metric}

The method performance evaluated on both synthetic and real-world dataset by panoramic error metric.

\textbf{Dataset}

% synthetic dataset
%And currently do not have any public optical flow equirectangular image dataset.
To quantitive evaluate the optical flow method, we generate a synthetic dataset include equirectangular RGB images and its ground truth equirectangular optical flow.
OpenGL is the most common rasterization rendering method, but the general OpenGL rendering pipeline doesn't support the panoramic camera model and optical flow generation.
So we implement the panoramic camera model on OpenGL geometry shaders with GLSL to transform the 3D mesh from Cartesian coordinate system to spherical coordinate system, finally render the 3D mesh to equirectangular format images.
The dataset's 3D data is from Replica ~\cite{??}, and \TODO{How much frame;What is the resolution;What kinds of motion it has?}

% real word dataset
The real-world data is more challenging, has complex lighting and motion etc. 
For further explorer our method performance,  we also test the method on the real-world dataset OmniPhoto ~\cite{??}, which is an open accessed dataset with diversity scene and captured with a high-quality panoramic camera.
Meanwhile, to evaluate on the read-word dataset without the ground truth optical flow, we use SSIM and RMS to metric between the RGB image and the optical flow warped RGB image ~\cite{???}.

\textbf{Error Metric}

It's very hard to measure the real word optical flow information, especially the equirectangular image. 
And currently do not have any public optical flow equirectangular image dataset.
So we evaluate our method estimated optical flow quantity in the synthetic equirectangular optical flow dataset.
Meanwhile to analysis the performance we use SSIM etc. metrics to evaluate the optical flow warped the equirectangular image sequence in both real world dataset and synthetic dataset.

The metrics AAE, EPE and RMS is used for the quantity analysis.
$n$ is the number of pixels, meanwhile set the unavailable pixels optical flow to 0 both ground truth and estimated optical flow.

The average endpoint error (\textbf{EPE}):  ~\cite{BakerSLRBS2011}
EPE is the absolute error of all the pixels.

Use the shortest path on unit sphere to compute the EPE.
\todolist{The greate circle equation.}

\begin{equation}\label{equ_exp_epe}
	E_{EPE} = \frac{1}{n} \sum_{i \in \Omega}\sqrt{(u_e^i - u_g^i)^2 + (v_e^i - v_g^i)^2}
\end{equation}

\begin{equation}\label{equ_exp_epe_sp}
	E_{EPEs} = \frac{1}{n} \sum_{i \in \Omega}\sqrt{(u_e^i - u_g^i)^2 + (v_e^i - v_g^i)^2}
\end{equation}

The average angular error (\textbf{AAE}):~\cite{??}
AAE is a metric to measure the deviation of angle:

\begin{equation}\label{equ_exp_aae}
	E_{AAE} = \frac{1}{n} \sum_{i \in \Omega}arctan(\frac{u^i_e v^i_g - u^i_g v^i_e}{u^i_e u^i_g + v^i_e v^i_g})
\end{equation}

\begin{equation}\label{equ_exp_aae_sp}
	E_{AAEs} = \frac{1}{n} \sum_{i \in \Omega}arctan(\frac{u^i_e v^i_g - u^i_g v^i_e}{u^i_e u^i_g + v^i_e v^i_g})
\end{equation}

The root mean square error (\textbf{RMSE}):~\cite{??}

\todolist{A diagram to show what the sphere version EPE and AAE like.}

\begin{equation}\label{equ_exp_rmse}
	E_{RMSE} = \sqrt{\frac{1}{n} \sum_{i \in \Omega}((u_e^i - u_g^i)^2 + (v_e^i - v_g^i)^2)}
\end{equation}

The ERP images introduce exaggerated distortion at north and south poles, which make a central angle error at spherical coordinate system in poles have much larger then the equator's.
To evaluate the error with same metric on the whole images, we extend EPE, AAE adn RMS to the spherical coordinate. 
Replace the euclidean distance and angle with the great circle distance (geodesics) and the angles of a spherical triangle. 
The error in spherical coordinate denote with $EPE_{cs}$, $AAE_{cs}$ and $RMS_{cs}$.

\subsection{Comparison}

We compare our method with following state of the art methods, FlowNet2~\cite{IlgMSKDB2017}, Dense Inverse Search (DIS) optical flow~\cite{KroegTDV2016} and PWC-Net~\cite{SunYLK2018}. 
The FlowNet2~\cite{IlgMSKDB2017} implementation is NVIDIA official release code \href{https://github.com/NVIDIA/flownet2-pytorch}{flownet2-pytorch}.
The PWC-Net~\cite{SunYLK2018}'s code is \href{https://github.com/NVlabs/PWC-Net}{NVlabs/PWC-Net}


\textbf{Quantitative Evaluation.}


The error in spherical coordinate shown as \cref{fig:exp:quality}.

\todolist{Test on and a table to list them:
\begin{enumerate}
\item whole dataset;
\item different kinds of camera motion and different magnitude motion;
\item different image resolution 0.5K, 1K, 2K.
\item Generalization: Compare the different opticalflow backbone result, to prove the generalisation of our method.
\end{enumerate}
}


\begin{table}[h!]
	\centering
	\begin{tabular}{ c | c | c | c | c | c | c }
		\hline
		& \multicolumn{6}{c}{DIS}  \\
		\hline
		& ${AAE}$ & ${EPE}$ & ${RMS}$ & ${AAE_{cs}}$ & ${EPE_{cs}}$ & ${RMS_{cs}}$ \\
		\hline
		apartment\_0 & - & - & -  & - & - & -  \\ 
		\hline
		hotel\_0 & - & - & -  & - & - & -  \\ 
		\hline
		office\_0 & - & - & - & - & - & -  \\ 
		\hline
		office\_4 & - & - & -  & - & - & -  \\ 
		\hline
		room\_0 & - & - & - & - & - & -  \\ 
		\hline
		room\_1 & - & - & -  & - & - & -  \\ 
		\hline\hline
	\end{tabular}
	\caption{Error of Replica dataset}
	\label{fig:exp:quality}
\end{table}


\todolist{analyze the error,where is larget, and why good and bad?}


\textbf{Qualitative Evaluation.}

\TODO{Compare on real-world dataset, OmniPhoto. The synthetic dataset just egomotion, the real-word dataset is including non-rigid and objects motion.}

\MZY{Our method can process the ego-motion very well, but maybe not good at other motion.}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.45\linewidth]{example-image}
	\caption{Optical Flow from different method.}
	\label{fig:exp:compflow}
\end{figure}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.45\linewidth]{example-image}
	\caption{Optical Flow Different method backward warp result.}
	\label{fig:exp:backwardwarp}
\end{figure}


\subsection{Ablations}

\textbf{Multi-step warp.}

Compare the result with and without the image alignment.
\todolist{How rotation affects accuracy?}

\textbf{Stitch blending weight.}

Test every terms of the weight,
