\section{Experiments}\label{sec:exp}

\textbf{Time} 

\todolist{How much time it will take for .}

\todolist{How to select the weight blending weight.}

\subsection{Dataset \& Metric}


\textbf{Dataset}

The dataset will be released for open access.


The synthetic panoramic optical flow rendered with on-the-shelf OpenGL render pipeline and store in the ERP image.
The input are textured 3D mesh and OpenGL's camera pose.
The process shown as the \cref{fig:approach:panoof:pipline} composing with the following 3 steps:

\begin{enumerate}
	\item Camera Model: OpenGL render with Equirectangular projection (ERP);
	\item Warp Around: Processing the warp around at the boundary of image; When project the sphere to ERP, the point along the plane x=0,y,-z are split to two side of ERP image, so the continued . It happend on theta (longitude), but in ERP image the phi (latitude) is continued, mean do not to consider/process the phi's warp around. \cref{fig:result:wraparound} (to warp a rainbow image to explain the wrap-around case.). So we use geodesic metric to replace the Cartesian distance.
	\item Occlusion: estimate the occlusion of optical flow (Do not talk it in this paper because the GT optical flow do not have perfect occlusion yet.);
	\item The panoramic motion ambiguity; In 360 scene, because the wrap-around we the pixel have two stright line path from source postion to target position, e.g. from left or right. In the paper we use the shortest path as the pixels optical flow path by default.
\end{enumerate}

The traditional OpenGL rendering pipeline doesn't support the panoramic camera model and optical flow generation.
For synthesising ground truth panoramic RGB images and optical flow, we implement and hire equirectangular perspective camera model.
The equirectangular perspective panoramic camera model render the 3D mesh to equirectangular images.
To achieve the camera model the OpenGL geometry shaders transform the 3D mesh from Cartesian coordinate system to spherical coordinate system. 

We also test our method on the real world dataset with warped image sequence.
It's very hard to measure the real word optical flow information, especially the equirectangular image. 
And currently do not have any public optical flow equirectangular image dataset.
So we evaluate our method estimated optical flow quantity in the synthetic equirectangular optical flow dataset.
Meanwhile to analysis the performance we use SSIM etc. metrics to evaluate the optical flow warped the equirectangular image sequence in both real world dataset and synthetic dataset.


\todolist{
\begin{enumerate}
	\item How much frame;
	\item What is the resolution;
	\item What kinds of motion it has?
\end{enumerate}
}


\textbf{Error Metric}

It's very hard to measure the real word optical flow information, especially the equirectangular image. 
And currently do not have any public optical flow equirectangular image dataset.
So we evaluate our method estimated optical flow quantity in the synthetic equirectangular optical flow dataset.
Meanwhile to analysis the performance we use SSIM etc. metrics to evaluate the optical flow warped the equirectangular image sequence in both real world dataset and synthetic dataset.

The metrics AAE, EPE and RMS is used for the quantity analysis.
$n$ is the number of pixels, meanwhile set the unavailable pixels optical flow to 0 both ground truth and estimated optical flow.

The average endpoint error (\textbf{EPE}):  ~\cite{BakerSLRBS2011}
EPE is the absolute error of all the pixels.

Use the shortest path on unit sphere to compute the EPE.
\todolist{The greate circle equation.}

\begin{equation}\label{equ_exp_epe}
	E_{EPE} = \frac{1}{n} \sum_{i \in \Omega}\sqrt{(u_e^i - u_g^i)^2 + (v_e^i - v_g^i)^2}
\end{equation}

\begin{equation}\label{equ_exp_epe_sp}
	E_{EPEs} = \frac{1}{n} \sum_{i \in \Omega}\sqrt{(u_e^i - u_g^i)^2 + (v_e^i - v_g^i)^2}
\end{equation}

The average angular error (\textbf{AAE}):~\cite{??}
AAE is a metric to measure the deviation of angle:

\begin{equation}\label{equ_exp_aae}
	E_{AAE} = \frac{1}{n} \sum_{i \in \Omega}arctan(\frac{u^i_e v^i_g - u^i_g v^i_e}{u^i_e u^i_g + v^i_e v^i_g})
\end{equation}

\begin{equation}\label{equ_exp_aae_sp}
	E_{AAEs} = \frac{1}{n} \sum_{i \in \Omega}arctan(\frac{u^i_e v^i_g - u^i_g v^i_e}{u^i_e u^i_g + v^i_e v^i_g})
\end{equation}

The root mean square error (\textbf{RMSE}):~\cite{??}

\todolist{A diagram to show what the sphere version EPE and AAE like.}

\begin{equation}\label{equ_exp_rmse}
	E_{RMSE} = \sqrt{\frac{1}{n} \sum_{i \in \Omega}((u_e^i - u_g^i)^2 + (v_e^i - v_g^i)^2)}
\end{equation}

The ERP images introduce exaggerated distortion at north and south poles, which make a central angle error at spherical coordinate system in poles have much larger then the equator's.
To evaluate the error with same metric on the whole images, we extend EPE, AAE adn RMS to the spherical coordinate. 
Replace the euclidean distance and angle with the great circle distance (geodesics) and the angles of a spherical triangle. 
The error in spherical coordinate denote with $EPE_{cs}$, $AAE_{cs}$ and $RMS_{cs}$.

\subsection{Comparison}

We compare our method with following state of the art methods, FlowNet2~\cite{IlgMSKDB2017}, Dense Inverse Search (DIS) optical flow~\cite{KroegTDV2016} and PWC-Net~\cite{SunYLK2018}. 
The FlowNet2~\cite{IlgMSKDB2017} implementation is NVIDIA official release code \href{https://github.com/NVIDIA/flownet2-pytorch}{flownet2-pytorch}.
The PWC-Net~\cite{SunYLK2018}'s code is \href{https://github.com/NVlabs/PWC-Net}{NVlabs/PWC-Net}


\textbf{Quantitative Evaluation.}


The error in spherical coordinate shown as \cref{fig:exp:quality}.

\todolist{Test on and a table to list them:
\begin{enumerate}
\item whole dataset;
\item different kinds of camera motion and different magnitude motion;
\item different image resolution 0.5K, 1K, 2K.
\item Generalization: Compare the different opticalflow backbone result, to prove the generalisation of our method.
\end{enumerate}
}


\begin{table}[h!]
	\centering
	\begin{tabular}{ c | c | c | c | c | c | c }
		\hline
		& \multicolumn{6}{c}{DIS}  \\
		\hline
		& ${AAE}$ & ${EPE}$ & ${RMS}$ & ${AAE_{cs}}$ & ${EPE_{cs}}$ & ${RMS_{cs}}$ \\
		\hline
		apartment\_0 & - & - & -  & - & - & -  \\ 
		\hline
		hotel\_0 & - & - & -  & - & - & -  \\ 
		\hline
		office\_0 & - & - & - & - & - & -  \\ 
		\hline
		office\_4 & - & - & -  & - & - & -  \\ 
		\hline
		room\_0 & - & - & - & - & - & -  \\ 
		\hline
		room\_1 & - & - & -  & - & - & -  \\ 
		\hline\hline
	\end{tabular}
	\caption{Error of Replica dataset}
	\label{fig:exp:quality}
\end{table}


\todolist{analyze the error,where is larget, and why good and bad?}


\textbf{Qualitative Evaluation.}

\TODO{Compare on real-world dataset, OmniPhoto. The synthetic dataset just egomotion, the real-word dataset is including non-rigid and objects motion.}

\MZY{Our method can process the ego-motion very well, but maybe not good at other motion.}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.45\linewidth]{example-image}
	\caption{Optical Flow from different method.}
	\label{fig:exp:compflow}
\end{figure}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.45\linewidth]{example-image}
	\caption{Optical Flow Different method backward warp result.}
	\label{fig:exp:backwardwarp}
\end{figure}


\subsection{Ablations}

\textbf{Multi-step warp.}

Compare the result with and without the image alignment.
\todolist{How rotation affects accuracy?}

\textbf{Stitch blending weight.}

Test every terms of the weight,
