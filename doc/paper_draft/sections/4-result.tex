\section{Experiments}
\label{sec:exp}

We evaluate our method on the Replica \cite{StrauWMCWGEMRVCYBYPYZLCBGMPSBSNGLN2019} and OmniPhotos \cite{BerteYLR2020} datasets and achieve state-of-the-art performance on both datasets.
%
%To uniformly evaluate each pixel optical flow error neglect the distortion and solve the wrap-around problem, we propose the spherical average endpoint error (SEPE), the spherical average angle error (SAAE), and the spherical root mean square error (SRMS).
To evaluate the accuracy of 360° optical flow regardless projection, and to account for the wrap-around (\cref{sec:approach:definition}), we measure the spherical average endpoint error (SEPE), spherical average angle error (SAAE), and the spherical root mean square error (SRMS).
%
%The method implemented in Python takes 20 seconds to estimate optical flow from a $1280 pixels\times640 pixels$ resolution image pairs.
%Our Python implementation uses DIS flow \cite{KroegTDV2016} for computing the tangent optical flow fields.
%Computing optical flow at 1280$\times$640 resolution takes about 20 seconds.
Our single-threaded Python implementation computes 360° optical flow at 1280$\times$640 resolution in about 20 seconds.
We use DIS flow \cite{KroegTDV2016} for the tangent flow fields.
\looseness-1


\subsection{Datasets \& Error Metrics}
\label{sec:datasets-metrics}

%To quantitative evaluate the optical flow method, we generate a synthetic dataset include equirectangular RGB images and its ground truth equirectangular optical flow.
%The method performance evaluated on both synthetic and real-world dataset by panoramic error metric.
We evaluate the performance of our method on synthetic and real-world datasets using multiple spherical error metrics.


\paragraph{Datasets.}
%% dicuess the 360 dataset
%
%% 360 dataset
%
%% synthetic dataset
%
%% 360 optical flow
Unlike depth sensors, %such as map sensor ToF camera or struct-light camera etc., 
there is no feasible hardware for capturing high-quality optical flow from a real-world scene, especially for 360° optical flow.
%
For quantitative evaluation of optical flow, ground truth is necessary.
%
Inspired by \citet{ShugrLKLSSF2019}, we render ground-truth 360° images and optical flow from the reconstructed 3D mesh of a read-world scene.
%% select the 3d mesh dataset
The ground-truth data is rendered using the Replica dataset \cite{StrauWMCWGEMRVCYBYPYZLCBGMPSBSNGLN2019}, which not only contains high-quality HDR textures and 3D meshes, but also public code for a versatile renderer.
%
%% how to modify the replica code
%OpenGL is the most common rasterization rendering method, but the general OpenGL rendering pipeline doesn't support the panoramic camera model and optical flow generation.
%So we implement the panoramic camera model on OpenGL geometry shaders with GLSL to transform the 3D mesh from Cartesian coordinate system to spherical coordinate system, finally render the 3D mesh to equirectangular format images.
However, the official Replica rendering pipeline does not support any 360° camera model or optical flow generation.
We therefore implemented an ERP camera model using OpenGL geometry shaders that transform the 3D mesh from Cartesian coordinates to spherical coordinates, finally renders the 3D mesh and ground-truth optical flow in the equirectangular format.
\looseness-1


%% The detail of the 360 replica optical flow dataset
%The dataset 3D data is from Replica \cite{StrauWMCWGEMRVCYBYPYZLCBGMPSBSNGLN2019}, and \TODO{How much frame;What is the resolution;What kinds of motion it has?}
%The 360° Replica dataset rendering with two kinds of camera path circle and line.
%The circle path (\emph{Circle}) is the camera face to outward and moves along a 0.5-meter radius circle, each image render by 10° interval.
%The line path (\emph{Line}) is the camera face the same direction and moves in a straight line, each image renders by 0.2-meter interval.
We render two types of camera paths for ground-truth evaluation.
A circular path (\emph{Circle}), in which the camera moves along a 50\,cm radius circle and faces outwards, with images rendered every 10°.
And a linear path (\emph{Line}), where the camera faces the same direction and moves in a straight line, with images rendered every 20\,cm.


%% real word dataset
%The synthetic dataset just ego-motion, the real-word dataset is including non-rigid and objects motion.
Although the synthetic dataset is highly realistic, real-world data tends to be more challenging, with complex lighting and motions.
%
To further explore our method's performance qualitatively, we also test our method on the OmniPhotos dataset \cite{BerteYLR2020}, which contains diverse 360° videos of real scenes captured with a high-quality 360° video camera.
%% how to evaluate
%Meanwhile, to evaluate on the read-word dataset without the ground truth optical flow, we use SSIM and RMS to metric between the RGB image and the optical flow warped RGB image ~\cite{???}.


\paragraph{Error Metrics.}

%The AAE, EPE and RMS ~\cite{BakerSLRBS2011} are the most common used for the perspective image optical flow quantity analysis, which metric the euclidean distance between the estimated optical flow and the ground truth optical flow.
Optical flow evaluation commonly uses the average angle error (AAE), end-point error (EPE), and root-mean-square error (RMS) metrics \cite{BakerSLRBS2011}.
% are the most common used for the perspective image optical flow quantity analysis, which metric the euclidean distance between the estimated optical flow and the ground truth optical flow.
%
%But the equirectangular image has distortion because closer top and bottom of image less space to corresponding one pixel.~\cite{??},
%To identically metric each pixel's equirectangular optical flow error, and neglect the equirectangular image distortion, when evaluate the ERP optical flow we map the equirectangular pixel coordinate to the spherical coordinates on the unit sphere, and use the geodesic distance to replace the Euclidean distance of metric.
To evaluate spherical 360° optical flow, we extend these metrics to the spherical domain by mapping ERP pixel coordinates to spherical coordinates on the unit sphere, and measuring geodesic distances instead of Euclidean distances in image space as used for perspective flow metrics.
This overcomes both distortions due to the equirectangular projection and also the wrap-around on the sphere.
%
%The ERP images introduce exaggerated distortion at north and south poles, which make a central angle error at spherical coordinate system in poles have much larger then the equator's.
%To evaluate the error with same metric on the whole images, we extend EPE, AAE and RMS to the spherical coordinate. 
%Replace the euclidean distance and angle with the great circle distance (geodesics) and the angles of a spherical triangle. 
%The error in spherical coordinate denote with $EPE_{cs}$, $AAE_{cs}$ and $RMS_{cs}$.
%
%, which metric on the spherical coordinate system. 
Specifically, we propose the spherical AAE (SAAE), spherical EPE (SEPE, see \cref{equ:exp:epesph}) and spherical RMS (SRMS) metrics to evaluate 360° optical flow.
The spherical AAE (SAAE) measures the angle between the estimated and ground-truth optical flow vectors on the surface of a unit sphere using spherical trigonometry.
%
%In both SEPE and SRMS the distance ($d$) is the geodesic distance between to endpoint on the unit sphere and the $n$ is the number of pixels.
For SEPE and SRMS, $d(\cdot, \cdot)$ is the geodesic distance between endpoints on the unit sphere, and the $N$ is the number of pixels:
%We set the unavailable optical flow to 0 both ground truth and estimated optical flow.
%
%\begin{equation}\label{equ_exp_epe}
%	E_{EPE} = \frac{1}{n} \sum_{i \in \Omega}\sqrt{(u_e^i - u_g^i)^2 + (v_e^i - v_g^i)^2}
%\end{equation}
\begin{equation}\label{equ:exp:epesph}
%	{SEPE} = \frac{1}{n} \sum_{i \in \Omega} d\left( (\theta^e_i,\phi^e_i), (\theta^g_i,\phi^g_i)\right)
	{SEPE} = \frac{1}{N} \sum_{i \in \Omega} d\left( (\theta^\text{est}_i, \phi^\text{est}_i), (\theta^\text{GT}_i, \phi^\text{GT}_i)\right)
	\text{.}
\end{equation}


% \cref{equ:exp:aaesph}
% \begin{equation}\label{equ_exp_aae}
% 	E_{AAE} = \frac{1}{n} \sum_{i \in \Omega}arctan(\frac{u^i_e v^i_g - u^i_g v^i_e}{u^i_e u^i_g + v^i_e v^i_g})
% \end{equation}
%\begin{equation}\label{equ:exp:aaesph}
%	\hat{E}_{AAEs} = \frac{1}{n} \sum_{i \in \Omega}arctan(\frac{u^i_e v^i_g - u^i_g v^i_e}{u^i_e u^i_g + v^i_e v^i_g})
%\end{equation}

%\TODO{Spherical version SSIM, which multiply a ERP weight, https://brilliant.org/wiki/surface-area-sphere/:Archimedes' Hat-Box Theorem}
%To metric the optical flow error of the real-world dataset use SSIM and The root mean square error (RMS) to calculate the error between the optical flow backwards warped target image $I_{t+1}$ and the source image $I_t$.~\cite{??}
%Meanwhile, to alleviate the equirectangular image distortion metric each pixel error based on the pixel area in the unit sphere.
%So scale the equirectangular each pixel SSIM with a normalized area weight.
% both real world dataset and synthetic dataset.

%\TODO{The equation to compute the weight.}
%% \begin{equation}\label{equ:exp:rmse}
%% 	E_{RMSE} = \sqrt{\frac{1}{n} \sum_{i \in \Omega}((u_e^i - u_g^i)^2 + (v_e^i - v_g^i)^2)}
%% \end{equation}
%\begin{equation}\label{equ:exp:rmsesph}
%	E_{RMSE} = w_{} \sqrt{\frac{1}{n} \sum_{i \in \Omega}((u_e^i - u_g^i)^2 + (v_e^i - v_g^i)^2)}
%\end{equation}

%\begin{figure}[hbt!]
%	\centering
%	\includegraphics[width=0.35\linewidth]{example-image}
%	\caption{A diagram to show what the sphere version EPE and AAE like. And the failure case of perspective image AAE and EPE. And the weight of RMSE and SSIM.}
%	\label{fig:exp:errormetric}
%\end{figure}


\subsection{Comparison}

We compare our method with following state-of-the-art methods: RAFT~\cite{TeedD2020a}, PWC-Net~\cite{SunYLK2018} and DIS~\cite{KroegTDV2016}.
%
For \href{https://github.com/princeton-vl/RAFT}{RAFT} and \href{https://github.com/NVlabs/PWC-Net}{PWC-Net}, we use the official released code, and for DIS, we use OpenCV's implementation.
% 
The performance evaluation comprises quantitative and qualitative evaluation on the Replica 360° and OmniPhotos datasets.


\paragraph{Quantitative Evaluation.}

%% how 
%The methods quantitative analysis result the \cref{tab:exp:oferrorquality} and \cref{fig:exp:compflow} shown the metric error number the pixel-wise error heatmap, respectively, are done on the Replica 360° dataset.
We show a quantitative comparison on the Replica 360° dataset using both perspective and spherical optical flow evaluation metrics in \cref{tab:exp:oferrorquality}.
%
%% analyse table
%\Cref{tab:exp:oferrorquality}
The table
%the perspective optical flow and 360° optical flow error metric used on the different camera path result,
shows that our method achieves the best performance across the board.
%
%In the $Circle$ path result, all methods are better than the \emph{Line} path's because there is more large pixel motion in the $Line$ path image pairs, make the optical flow hard to find the pixel's corresponding relationship between two images.
All methods tend to be better on the \emph{Circle} paths than \emph{Line} paths, as the latter contains larger displacements that are more difficult to estimate accurately.
%
Meanwhile, our global rotation warping pre-aligns the input images, so that our method more easily handles camera ego-motion.
%By the way, as shown in the DIS's $Line$ result, its EPE is the largest but its SEPE is less than PWCNet's SEPE, 
%the SEPE metric the error with geodesic distance to neglect the 360° image distortion, so the SEPE is more objective than EPE.


\Cref{fig:exp:compflow} shows some example estimated 360° optical flow fields and their SEPE visualised as heatmaps (lighter is better).
%% analyse the diagram
%The optical flow SEPE heatmap shown in the 2nd than 4th rows of \cref{fig:exp:compflow}, there are visualized in the same error range and larger error darker.
Other methods mainly introduce errors on the top and bottom edges of the optical flow.
Our method has the lowest errors at the top and bottom, while the middle region is quite similar to the DIS result.
%The RAFT the best performance in the middle of the image.


%% summary
Based on the \cref{tab:exp:oferrorquality} and \cref{fig:exp:compflow}, we can conclude that
%
perspective optical flow cannot correctly handle the top and bottom regions of ERP images, but our method can help them improve dramatically in these areas.
%
Our method succeeds where the underlying optical flow method (DIS) fails, while maintaining the same performance in the equatorial image region.



%% TODO 
%different image resolution 0.5K, 1K, 2K.
%Generalization: Compare the different optical flow backbone result, to prove the generalisation of our method.
\begin{table}[h!]
	\centering
	\caption{\label{tab:exp:oferrorquality}%
		Optical flow errors on the synthetic Replica 360° dataset.
		See \cref{sec:datasets-metrics} for metrics.}
	\resizebox{0.8\linewidth}{!}{%
	%	\setlength{\tabcolsep}{3pt}%
	\begin{tabular}{p{0.2cm}p{1.6cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.3cm}p{1.3cm}p{1.3cm}}
		\toprule
		&  Method & ${EPE}\downarrow$ & $AAE\downarrow$ & $RMS\downarrow$ & $SEPE\downarrow$ & ${SAAE}\downarrow$ & ${SRMS}\downarrow$ \\
		\midrule
		\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{Circle}}}
		& PWC-Net &               16.60  &     0.2994 &     63.44 &     0.0588   &     0.1469  &     0.3845  \\ 
		& RAFT    &               16.56  &     0.2775 &     64.94 &     0.04669  &     0.1482  &     0.3301  \\
		& DIS     &               16.50  &     0.2881 &     62.67 &     0.05375  &     0.1566  &     0.3515  \\
		& Ours    & \bf \phantom{0}4.988 & \bf 0.1885 & \bf 39.26 & \bf 0.007241 & \bf 0.05278 & \bf 0.01824 \\ 
		\midrule
		\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{Line}}}
		& PWC-Net &           32.25  &     0.2139 & 93.32     &     0.1215  &     0.1429  &     0.5289  \\ 
		& RAFT    &           31.75  &     0.2908 & 96.55     &     0.08752 &     0.1595  &     0.4201  \\ 
		& DIS     &           35.55  &     0.3328 & 98.24     &     0.1149  &     0.2063  &     0.4768  \\
		& Ours    &    \bf    10.41  & \bf 0.2461 & \bf 55.78 & \bf 0.01533 & \bf 0.08943 & \bf 0.03448 \\
		\midrule
		\multirow{4}{*}{\rotatebox[origin=c]{90}{\emph{All}}} 
		& PWC-Net &               18.95  &     0.2866 &     67.92 &     0.06822 &     0.1463  &     0.4062  \\
		& RAFT    &               20.20  &     0.2807 &     72.53 &     0.05648 &     0.1509  &     0.3517  \\
		& DIS     &               25.74  &     0.3045 &     79.94 &     0.08347 &     0.1797  &     0.4127  \\
		& Ours    & \bf \phantom{0}8.602 & \bf 0.2270 & \bf 50.27 & \bf 0.01264 & \bf 0.07722 & \bf 0.02907 \\
		\bottomrule
	\end{tabular}}%
\end{table}


%\begin{table}[h!]
%	\centering
%	\caption{\label{fig:exp:warpingerrorquality}%
%		Result of optical flow warped image error on Replica and Omniphoto dataset.}
%	\begin{tabular}{c c c c c c c c}
%		\hline
%		Dataset &  Method & ${RMS}$ & $SSIM$ & $SRMS$ & $SSSIM$ \\
%		\hline
%		\multirow{3}{*}{Replica} & PWCNet & - & -  & - & - \\ 
%		& RAFT & - & -  & - & - \\ 
%		& Our & - & -  & - & -  \\ 
%		\hline
%		\multirow{3}{*}{OmniPhoto} & PWCNet & - & -  & - & - \\ 
%		& RAFT & - & -  & - & - \\ 
%		& Our & - & -  & - & - \\ 
%		\hline
%	\end{tabular}
%\end{table}


\newcommand{\labeledfig}[4]{
	\begin{tikzpicture}
		\draw node[name=micrograph] {\includegraphics[width=#2\textwidth]{#1}}; %the image
		\draw (micrograph.north west)  node[anchor=north west,yshift=-2,#4]{\textbf{\tiny{#3}}}; %image label
	\end{tikzpicture}
}

\begin{figure*}[hbt!]
	\centering
	\begin{tabular}{@{}c*{5}{@{\hspace{-9pt}}c}@{}}
		\labeledfig{images/of_error/office_0_line_1k_0_dis_0002_rgb_pano.jpg}{0.185}{(a) $I_t$}{white} &
		\labeledfig{images/of_error/office_0_line_1k_0_dis_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(b) $DIS$}{white} &
		\labeledfig{images/of_error/office_0_line_1k_0_our_weight_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(c) $Our$}{white} &
		\labeledfig{images/of_error/office_0_line_1k_0_pwcnet_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(d) $PWCNet$}{white} &
		\labeledfig{images/of_error/office_0_line_1k_0_raft_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(e) $RAFT$}{white}
		\\[-1.0em]
		\labeledfig{images/of_error/office_0_line_1k_0_gt_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(f) GT}{white} &
		\labeledfig{images/of_error/office_0_line_1k_0_dis_0002_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(g) $DIS\_SEPE$}{black} &
		\labeledfig{images/of_error/office_0_line_1k_0_our_weight_0002_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(h) $Our\_SEPE$}{black} &
		\labeledfig{images/of_error/office_0_line_1k_0_pwcnet_0002_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(i) $PWCNet\_SEPE$}{black} &
		\labeledfig{images/of_error/office_0_line_1k_0_raft_0002_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(j) $RAFT\_SEPE$}{black}
		\\[-0.9em]
		\labeledfig{images/of_error/apartment_0_circ_1k_0_0004_rgb_pano.jpg}{0.185}{(a) $I_t$}{white} &
		\labeledfig{images/of_error/apartment_0_circ_1k_0_dis_0004_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(b) $DIS$}{black} &
		\labeledfig{images/of_error/apartment_0_circ_1k_0_our_weight_0004_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(c) $Our$}{black} &
		\labeledfig{images/of_error/apartment_0_circ_1k_0_pwcnet_0004_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(d) $PWCNet$}{black} &
		\labeledfig{images/of_error/apartment_0_circ_1k_0_raft_0004_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(e) $RAFT$}{black}
		\\[-1.0em]
		\labeledfig{images/of_error/apartment_0_circ_1k_0_gt_0004_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(f) GT}{black} &
		\labeledfig{images/of_error/apartment_0_circ_1k_0_dis_0004_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(g) $DIS\_SEPE$}{black} &
		\labeledfig{images/of_error/apartment_0_circ_1k_0_our_weight_0004_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(h) $Our\_SEPE$}{black} &
		\labeledfig{images/of_error/apartment_0_circ_1k_0_pwcnet_0004_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(i) $PWCNet\_SEPE$}{black} &
		\labeledfig{images/of_error/apartment_0_circ_1k_0_raft_0004_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(j) $RAFT\_SEPE$}{black}
		%		\\[-0.9em]
		%		\labeledfig{images/of_error/room_0_circ_1k_0002_rgb_pano.jpg}{0.185}{(a) $I_t$}{white} &
		%		\labeledfig{images/of_error/room_0_circ_1k_0_dis_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(b) $DIS$}{black} &
		%		\labeledfig{images/of_error/room_0_circ_1k_0_our_weight_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(c) $Our$}{black} &
		%		\labeledfig{images/of_error/room_0_circ_1k_0_pwcnet_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(d) $PWCNet$}{black} &
		%		\labeledfig{images/of_error/room_0_circ_1k_0_raft_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(e) $RAFT$}{black}
		%		\\[-1.0em]
		%		\labeledfig{images/of_error/room_0_circ_1k_0_gt_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(f) GT}{black} &
		%		\labeledfig{images/of_error/room_0_circ_1k_0_dis_0002_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(g) $DIS\_SEPE$}{black} &
		%		\labeledfig{images/of_error/room_0_circ_1k_0_our_weight_0002_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(h) $Our\_SEPE$}{black} &
		%		\labeledfig{images/of_error/room_0_circ_1k_0_pwcnet_0002_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(i) $PWCNet\_SEPE$}{black} &
		%		\labeledfig{images/of_error/room_0_circ_1k_0_raft_0002_opticalflow_backward_pano.flo_error_vis.jpg}{0.185}{(j) $RAFT\_SEPE$}{black}
	\end{tabular}
	\caption{\label{fig:exp:compflow}%
		Estimated 360° optical flow and error heatmaps on the Replica 360° dataset:
		(a) Source image.
		(b–e) Estimated flow fields.
		(f) Ground-truth flow.
		(g–j) SEPE (spherical end-point error) heatmaps (lighter is better).
		Top: \emph{office\_0} scene.
		Bottom: \emph{apartment\_0} scene.}
\end{figure*}


\paragraph{Qualitative Evaluation.}

To analyse the performance of 360° optical flow methods on a real-world dataset without ground-truth optical flow, we measure the interpolation error \cite{BakerSLRBS2011}, i.e. the RGB colour difference between the source image and backward-warped target image.
%
\Cref{fig:exp:backwardwarp} shows an example result from the OmniPhotos dataset.
The error maps in \Cref{fig:exp:backwardwarp}(k–n) show less error at the top and bottom of images for our method's result.
%The OmniPhoto difference is not obvious as the Replica 360°'s because the OmniPhoto is a high frame rate dataset there is less camera motion between the two sequence images.
The differences between methods is less pronounced than in the Replica 360° dataset, as the baseline between frames is smaller in OmniPhotos due to a higher video frame rate.


\begin{figure}[hbt!]
	\centering
	\begin{tabular}{@{}c*{5}{@{\hspace{-8pt}}c}@{}}
%	\labeledfig{images/of_warp/room_1_circ_1k_0_0001_rgb_pano.jpg}{0.185}{(a) $I_t$}{white} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_dis_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(b) DIS}{black} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_our_weight_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(c) Our}{black} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_pwcnet_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(d) PWCNet}{black} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_raft_0002_opticalflow_backward_pano.flo_flow_vis.jpg}{0.185}{(e) RAFT}{black}
%	\\[-1.0em]
%	\labeledfig{images/of_warp/room_1_circ_1k_0_0002_rgb_pano.jpg}{0.185}{(f) $I_{t+1}$}{black} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_dis_0002_opticalflow_backward_pano.flo_wrap.jpg}{0.185}{(g) DIS(Warp)}{black} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_our_weight_0002_opticalflow_backward_pano.flo_wrap.jpg}{0.185}{(h) Our(Warp)}{black} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_pwcnet_0002_opticalflow_backward_pano.flo_wrap.jpg}{0.185}{(i) PWCNet(Warp)}{black} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_raft_0002_opticalflow_backward_pano.flo_wrap.jpg}{0.185}{(j) RAFT(Warp)}{black}
%	\\[-1.0em]
%	 &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_dis_0002_opticalflow_backward_pano.flo_warp_error_vis.jpg}{0.185}{(k) DIS(Warp)}{black} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_our_weight_0002_opticalflow_backward_pano.flo_warp_error_vis.jpg}{0.185}{(l) Our(Warp)}{black} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_pwcnet_0002_opticalflow_backward_pano.flo_warp_error_vis.jpg}{0.185}{(m) PWCNet(Warp)}{black} &
%	\labeledfig{images/of_warp/room_1_circ_1k_0_raft_0002_opticalflow_backward_pano.flo_warp_error_vis.jpg}{0.185}{(n) RAFT(Warp)}{black}
%	\\[-0.9em]
	\labeledfig{images/of_warp/BeihaiPark_panoramic-0549.jpg}{0.185}{(a) $I_t$}{black} &
	\labeledfig{images/of_warp/BeihaiPark_dis_0549_opticalflow_forward_pano.flo_flow_vis.jpg}{0.185}{(b) DIS}{black} &
	\labeledfig{images/of_warp/BeihaiPark_our_weight_0549_opticalflow_forward_pano.flo_flow_vis.jpg}{0.185}{(c) Our}{black} &
	\labeledfig{images/of_warp/BeihaiPark_pwcnet_0549_opticalflow_forward_pano.flo_flow_vis.jpg}{0.185}{(d) PWCNet}{black} &
	\labeledfig{images/of_warp/BeihaiPark_raft_0549_opticalflow_forward_pano.flo_flow_vis.jpg}{0.185}{(e) RAFT}{black}
	\\[-1.0em]
	\labeledfig{images/of_warp/BeihaiPark_panoramic-0550.jpg}{0.185}{(f) $I_{t+1}$}{black} &
	\labeledfig{images/of_warp/BeihaiPark_dis_0549_opticalflow_forward_pano.flo_wrap.jpg}{0.185}{(g) DIS(Warp)}{black} &
	\labeledfig{images/of_warp/BeihaiPark_our_weight_0549_opticalflow_forward_pano.flo_wrap.jpg}{0.185}{(h) Our(Warp)}{black} &
	\labeledfig{images/of_warp/BeihaiPark_pwcnet_0549_opticalflow_forward_pano.flo_wrap.jpg}{0.185}{(i) PWCNet(Warp)}{black} &
	\labeledfig{images/of_warp/BeihaiPark_raft_0549_opticalflow_forward_pano.flo_wrap.jpg}{0.185}{(j) RAFT(Warp)}{black}
	\\[-1.0em]
	&
	\labeledfig{images/of_warp/BeihaiPark_dis_0549_opticalflow_forward_pano.flo_warp_error_vis.jpg}{0.185}{(k) DIS(Warp)}{black} &
	\labeledfig{images/of_warp/BeihaiPark_our_weight_0549_opticalflow_forward_pano.flo_warp_error_vis.jpg}{0.185}{(l) Our(Warp)}{black} &
	\labeledfig{images/of_warp/BeihaiPark_pwcnet_0549_opticalflow_forward_pano.flo_warp_error_vis.jpg}{0.185}{(m) PWCNet(Warp)}{black} &
	\labeledfig{images/of_warp/BeihaiPark_raft_0549_opticalflow_forward_pano.flo_warp_error_vis.jpg}{0.185}{(n) RAFT(Warp)}{black}
\end{tabular}
	\caption{\label{fig:exp:backwardwarp}%
		Backward warping results on OmniPhotos' \emph{BeihaiPark} scene:
		(a, f) Source and target images.
		(b–e) Estimated flow fields.
		(g–j) Target image (f) warped to the source image (a) using the estimated optical flow (b–e).
		(k–n) Interpolation error heatmaps (lighter is better).}
\end{figure}


\subsection{Ablation Studies}
\label{sec:ablations}

\paragraph{Global Rotation Warping.}
The multi-step rotation warping aligns image $I_{t+1}$ to $I_t$ by estimating rotation using the ERP and cubemap optical flow.
The results without ERP optical flow alignment (`w/o ERP') and without cubemap optical flow alignment (`w/o cubemap') are shown in \cref{fig:exp:ablations}.
%%{analysis result}
The `w/o ERP' error is higher than the `w/o cubemap' error for both `\emph{Line}' and `\emph{All}' data,
%because the ERP optical flow is the global motion, which can handle large pixel motion.
as the ERP-based rotation estimation better handles large pixel motions.


\paragraph{Stitch Blending Weight.}
The blending weight aims to smoothly and consistently stitch the overlap area of different faces' optical flow.
The result without blending weight is shown as `w/o weight' in \cref{fig:exp:ablations}, which replaces all weights with the unit weight.
%{analysis result}
%The $W/oWeight$ get the best performance in $Line$ because the blending weight helps the optical flow stitch to get rid of the error optical flow generated by large pixel motion.
Our full method performs best for `\emph{Line}' data as helps the optical flow stitching overcome errors caused by large pixel motion.
However, `w/o weight' performs slightly better on the complete dataset.


\begin{table}[h!]
	\centering
	\caption{\label{fig:exp:ablations}%
		The results of our ablation study. See \cref{sec:ablations} for details.}
	\resizebox{0.8\linewidth}{!}{%
%	\setlength{\tabcolsep}{3pt}%
	\begin{tabular}{p{0.2cm}p{1.9cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.3cm}p{1.3cm}p{1.3cm}}
		\toprule
		&  Method & ${EPE}\downarrow$ & $AAE\downarrow$ & $RMS\downarrow$ & $SEPE\downarrow$ & ${SAAE}\downarrow$ & ${SRMS}\downarrow$ \\
		\midrule
		\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{Circle}}}
		& Full method  &     4.988 &     0.1885 &     39.26 &     0.007241 &     0.05278 &     0.01824 \\ 
		& w/o weight   & \bf 4.854 & \bf 0.1863 & \bf 37.10 & \bf 0.007118 & \bf 0.05074 &     0.01899 \\
		& w/o ERP      &     4.961 &     0.1871 &     38.32 &     0.007165 &     0.05094 &     0.01504 \\ 
		& w/o cubemap  &     4.956 &     0.1881 &     38.22 &     0.007171 &     0.05241 & \bf 0.01451 \\ 
		\midrule
		\multirow{3}{*}{\rotatebox[origin=c]{90}{\emph{Line}}} 
		& Full method  &     10.41 &     0.2461 &     55.78 & \bf 0.01533 &     0.08943 & \bf 0.03448 \\
		& w/o weight   & \bf 10.13 &     0.2447 & \bf 48.60 &     0.01600 & \bf 0.08798 &     0.03566 \\ 
		& w/o ERP      &     10.82 & \bf 0.2648 &     56.48 &     0.01580 &     0.09090 &     0.03519 \\ 
		& w/o cubemap  &     10.88 &     0.2484 &     57.61 &     0.01575 &     0.08987 &     0.03535 \\ 
		\midrule
		\multirow{4}{*}{\rotatebox[origin=c]{90}{\emph{All}}}
		& Full method  &     8.602 &     0.2270 &     50.27 &     0.01264 &     0.07722 &     0.02907 \\
		& w/o weight   & \bf 7.524 & \bf 0.2130 & \bf 42.94 &     0.01161 & \bf 0.06967 &     0.02741 \\
		& w/o ERP      &     7.889 &     0.2260 &     47.40 &     0.01149 &     0.07092 &     0.02512 \\
		& w/o cubemap  &     7.921 &     0.2182 &     47.91 & \bf 0.01146 &     0.07114 & \bf 0.02492 \\
		\bottomrule
	\end{tabular}}%
\end{table}\vspace{-2em}
